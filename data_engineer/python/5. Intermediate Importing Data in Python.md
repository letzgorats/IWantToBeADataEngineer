## 🌐 파이썬으로 웹에서 데이터 가져오기: urllib, requests, BeautifulSoup 완전 정복
데이터 분석의 첫 걸음은 데이터 수집입니다. 이제 로컬 파일뿐 아니라 웹에서도 직접 데이터를 가져오는 방법을 배워야 할 때입니다. 
이번 포스팅에서는 **실제로 웹에서 데이터를 자동으로 가져오고 HTML을 파싱하는 방법** 을 실습해봅니다.(본 내용은 DataCamp의 Intermediate Importing Data in Python 강의의 첫 번째 챕터 내용을 정리한 것입니다.)

### 📦 1. 웹에서 파일 다운로드하기

#### ✅ 문제: 웹에서 csv 파일을 다운받고 싶은데?
예를 들어, UCI에서 제공하는 와인 품질 데이터셋 중 화이트 와인 데이터를 가져오고 싶다고 해봅시다.

🧪 예제: urllib로 파일 저장하기
```python
from urllib.request import urlretrieve

url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
urlretrieve(url, 'winequality-white.csv')
```
> urlretrieve() 함수는 해당 URL의 내용을 로컬 파일로 바로 저장해줍니다.

> 코드로 작성해두면 재현성과 확장성을 확보할 수 있어 협업에 유리합니다.

### 📄 2. pandas로 웹에서 직접 파일 읽기
굳이 로컬에 저장하지 않아도 바로 DataFrame으로 가져올 수 있습니다.

```python
import pandas as pd

df = pd.read_csv(url)
print(df.head())
```
> 아주 깔끔하죠? `pandas`가 `http/https` 경로도 인식합니다.

### 🌐 3. HTTP 요청의 기본 이해

#### 🧠 URL이란?  
- `U`niform/Universe `R`esource `L`ocator 의 약자로 웹 리소스에 대한 참조일 뿐입니다.
- 즉, FTP(파일전송 프로토콜)나 데이터 베이스 엑세스 등 몇 가지 다른 것을 지칭할 수도 있지만, 그 중에서 웹 사이트의 위치인 `웹 주소`라는 것에 집중해봅시다.
- https://datacamp.com 같은 웹 주소는 사실 서버에게 보내는 HTTP 요청의 일종입니다.
  - 여기서 `http/https`는 `프로토콜 식별자`(Protocol identifier) 이고,(`http`는 `HyperText Transfer Protocol`를 의미합니다.)
  - `datacamp.com`은 `리소스 이름`(Resource name) 입니다.
- 웹 사이트를 접속할 때마다 실제로는 서버에 HTTP 요청을 보내는 것으로, 가장 흔한 요청은 `GET` 요청으로, 데이터를 `'가져오라'`는 의미입니다.

📌 urllib로 GET 요청 보내기
```python
from urllib.request import Request, urlopen

url = 'https://en.wikipedia.org/wiki/Main_Page'
req = Request(url)
res = urlopen(req) # urlopen을 통해 HttpResponse 객체를 반환
html = res.read() # html을 문자열로 반환
res.close() # response 닫기 잊으면 안된다!
print(html[:500])
```
> `.read()` 를 통해 HTML 원문을 문자열로 가져올 수 있습니다.

### 🚀 4. requests: 더 쉬운 HTTP 요청

```python
import requests

res = requests.get('https://en.wikipedia.org/wiki/Main_Page')
html = res.text
print(html[:500])
```
> 훨씬 짧고 직관적이죠? `.get()` 메서드만으로 응답 객체인 HTML을 얻을 수 있다.

> 실제로 `requests` 는 Python 에서 가장 인기 있는 `HTTP 라이브러리` 중 하나입니다.

### 🍜 5. 웹 페이지 스크래핑: BeautifulSoup

#### 🔍 HTML에서 필요한 정보만 뽑고 싶다면?
HTML(HyperText Markup Language)은 반정형 데이터입니다. `태그(<h1>, <a>, <p>)` 를 기준으로 데이터를 구조화할 수 있습니다.

🥣 BeautifulSoup 기본 사용법
```python
import requests
from bs4 import BeautifulSoup

url = 'https://www.crumy.com/software/BeautifulSoup/'
r = requests.get(url)
html_doc = r.text
soup = BeautifulSoup(html_doc, 'html.parser')  # beautilfulsoup 을 통해 beautiful 한 구조로 만든다. (제대로 작성된 html 처럼 들여쓰기도 잘 되어있다.)('html.parser' 없어도 된다.(디폴트))
print(soup.prettify()[:500])
```

> `prettify()`는 HTML을 예쁘게 정리해줍니다. HTML 구조를 파악하는 데 유용하죠.

### 🎯 6. 웹 페이지에서 정보 추출하기

#### 🏷️ 예: 페이지 제목 가져오기
```python
print(soup.title.get_text())
```

#### 🔗 모든 하이퍼링크 추출
```python
for link in soup.find_all('a'):
    print(link.get('href'))
```
- `.find_all('a')` : 모든 `<a>` (하이퍼링크) 태그를 가져옵니다.
- `.get('href')` : 해당 태그의 링크 주소만 추출합니다.

### 📚 정리: 웹 데이터 수집 파이프라인 요약

| 단계 | 기술                  | 설명                               |
| -- | ------------------- | -------------------------------- |
| 1  | `urllib`            | URL에서 파일 직접 다운로드 (`urlretrieve`) |
| 2  | `pandas.read_csv()` | 웹의 csv 파일을 바로 DataFrame으로 불러오기   |
| 3  | `requests`          | 간단한 GET 요청 처리                    |
| 4  | `BeautifulSoup`     | HTML 문서 파싱 및 정보 추출               |


#### 📌 추천 실습 URL
- [UCI Wine Quality Dataset (Red)](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv)
- [Wikipedia Main Page](https://en.wikipedia.org/wiki/Main_Page)

### 마무리 
웹에서 데이터를 가져오는 기술은 데이터 수집 자동화, 크롤러 개발, ETL 파이프라인 구성의 핵심입니다.
이 글을 통해 urllib, requests, BeautifulSoup의 기본기를 다졌다면, 다음 단계로는 **동적 웹 크롤링(Selenium)** 이나 **API 활용(json, RESTful API)** 로 나아갈 수 있습니다.

<hr>

## 









## 🌐 파이썬으로 웹에서 데이터 가져오기: urllib, requests, BeautifulSoup 완전 정복
데이터 분석의 첫 걸음은 데이터 수집입니다. 이제 로컬 파일뿐 아니라 웹에서도 직접 데이터를 가져오는 방법을 배워야 할 때입니다. 
이번 포스팅에서는 **실제로 웹에서 데이터를 자동으로 가져오고 HTML을 파싱하는 방법** 을 실습해봅니다.(본 내용은 DataCamp의 Intermediate Importing Data in Python 강의의 첫 번째 챕터 내용을 정리한 것입니다.)

### 📦 1. 웹에서 파일 다운로드하기

#### ✅ 문제: 웹에서 csv 파일을 다운받고 싶은데?
예를 들어, UCI에서 제공하는 와인 품질 데이터셋 중 화이트 와인 데이터를 가져오고 싶다고 해봅시다.

🧪 예제: urllib로 파일 저장하기
```python
from urllib.request import urlretrieve

url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
urlretrieve(url, 'winequality-white.csv')
```
> urlretrieve() 함수는 해당 URL의 내용을 로컬 파일로 바로 저장해줍니다.

> 코드로 작성해두면 재현성과 확장성을 확보할 수 있어 협업에 유리합니다.

### 📄 2. pandas로 웹에서 직접 파일 읽기
굳이 로컬에 저장하지 않아도 바로 DataFrame으로 가져올 수 있습니다.

```python
import pandas as pd

df = pd.read_csv(url)
print(df.head())
```
> 아주 깔끔하죠? `pandas`가 `http/https` 경로도 인식합니다.

### 🌐 3. HTTP 요청의 기본 이해

#### 🧠 URL이란?  
- `U`niform/Universe `R`esource `L`ocator 의 약자로 웹 리소스에 대한 참조일 뿐입니다.
- 즉, FTP(파일전송 프로토콜)나 데이터 베이스 엑세스 등 몇 가지 다른 것을 지칭할 수도 있지만, 그 중에서 웹 사이트의 위치인 `웹 주소`라는 것에 집중해봅시다.
- https://datacamp.com 같은 웹 주소는 사실 서버에게 보내는 HTTP 요청의 일종입니다.
  - 여기서 `http/https`는 `프로토콜 식별자`(Protocol identifier) 이고,(`http`는 `HyperText Transfer Protocol`를 의미합니다.)
  - `datacamp.com`은 `리소스 이름`(Resource name) 입니다.
- 웹 사이트를 접속할 때마다 실제로는 서버에 HTTP 요청을 보내는 것으로, 가장 흔한 요청은 `GET` 요청으로, 데이터를 `'가져오라'`는 의미입니다.

📌 urllib로 GET 요청 보내기
```python
from urllib.request import Request, urlopen

url = 'https://en.wikipedia.org/wiki/Main_Page'
req = Request(url)
res = urlopen(req) # urlopen을 통해 HttpResponse 객체를 반환
html = res.read() # html을 문자열로 반환
res.close() # response 닫기 잊으면 안된다!
print(html[:500])
```
> `.read()` 를 통해 HTML 원문을 문자열로 가져올 수 있습니다.

### 🚀 4. requests: 더 쉬운 HTTP 요청

```python
import requests

res = requests.get('https://en.wikipedia.org/wiki/Main_Page')
html = res.text
print(html[:500])
```
> 훨씬 짧고 직관적이죠? `.get()` 메서드만으로 응답 객체인 HTML을 얻을 수 있다.

> 실제로 `requests` 는 Python 에서 가장 인기 있는 `HTTP 라이브러리` 중 하나입니다.

### 🍜 5. 웹 페이지 스크래핑: BeautifulSoup

#### 🔍 HTML에서 필요한 정보만 뽑고 싶다면?
HTML(HyperText Markup Language)은 반정형 데이터입니다. `태그(<h1>, <a>, <p>)` 를 기준으로 데이터를 구조화할 수 있습니다.

🥣 BeautifulSoup 기본 사용법
```python
import requests
from bs4 import BeautifulSoup

url = 'https://www.crumy.com/software/BeautifulSoup/'
r = requests.get(url)
html_doc = r.text
soup = BeautifulSoup(html_doc, 'html.parser')  # beautilfulsoup 을 통해 beautiful 한 구조로 만든다. (제대로 작성된 html 처럼 들여쓰기도 잘 되어있다.)('html.parser' 없어도 된다.(디폴트))
print(soup.prettify()[:500])
```

> `prettify()`는 HTML을 예쁘게 정리해줍니다. HTML 구조를 파악하는 데 유용하죠.

### 🎯 6. 웹 페이지에서 정보 추출하기

#### 🏷️ 예: 페이지 제목 가져오기
```python
print(soup.title.get_text())
```

#### 🔗 모든 하이퍼링크 추출
```python
for link in soup.find_all('a'):
    print(link.get('href'))
```
- `.find_all('a')` : 모든 `<a>` (하이퍼링크) 태그를 가져옵니다.
- `.get('href')` : 해당 태그의 링크 주소만 추출합니다.

### 📚 정리: 웹 데이터 수집 파이프라인 요약

| 단계 | 기술                  | 설명                               |
| -- | ------------------- | -------------------------------- |
| 1  | `urllib`            | URL에서 파일 직접 다운로드 (`urlretrieve`) |
| 2  | `pandas.read_csv()` | 웹의 csv 파일을 바로 DataFrame으로 불러오기   |
| 3  | `requests`          | 간단한 GET 요청 처리                    |
| 4  | `BeautifulSoup`     | HTML 문서 파싱 및 정보 추출               |


#### 📌 추천 실습 URL
- [UCI Wine Quality Dataset (Red)](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv)
- [Wikipedia Main Page](https://en.wikipedia.org/wiki/Main_Page)

### 마무리 
웹에서 데이터를 가져오는 기술은 데이터 수집 자동화, 크롤러 개발, ETL 파이프라인 구성의 핵심입니다.
이 글을 통해 urllib, requests, BeautifulSoup의 기본기를 다졌다면, 다음 단계로는 **동적 웹 크롤링(Selenium)** 이나 **API 활용(json, RESTful API)** 로 나아갈 수 있습니다.

<hr>

## 🔌 파이썬으로 API 데이터 가져오기: JSON, 쿼리스트링, 실전 사용
API는 애플리케이션 간에 표준화된 방식으로 데이터를 주고받기 위한 인터페이스입니다. 
이번 글에서는 **`JSON 포맷 이해` → `Python으로 API 호출` → `응답 파싱` → `에러/속도/페이지네이션 처리`** 까지 한 번에 정리합니다. 예제 API로 **OMDb(영화 정보)** 와 **Library of Congress(LoC)** 를 사용합니다.

### 1. Introduction to APIs and JSONs

#### 🔍 API란?
- **API (Application Programming Interface)** : 소프트웨어 간 통신을 위한 프로토콜과 규칙의 집합.
- 예: Twitter API로 트윗 데이터 수집, Wikipedia API로 문서 정보 가져오기.
- REST API는 **HTTP 요청(GET, POST 등)** 을 통해 데이터를 주고받음.

#### 📄 JSON이란?
- **JavaScript Object Notation**: 경량 데이터 교환 포맷.
- **사람이 읽기 쉬움 + 기계가 처리하기 쉬움.**
- 구조:
  - { "key": "value", "year": "2006" } 형태의 `키-값` 쌍
  - 값은 문자열, 숫자, 배열, 객체(JSON 중첩) 가능.
- 파이썬에서 dict로 자연스럽게 매핑됨.

#### 🧪 로컬 JSON 불러오기 예제
```python
import json

# 로컬 JSON 파일 읽기
with open('snakes.json', 'r', encoding='utf-8') as file:
    json_data = json.load(file)

print(type(json_data))  # dict
for key, value in json_data.items():
    print(key, ":", value)
```
> Context Manager 내부에서 `.load()`를 활용하면 됩니다.

### 2. APIs and Interacting with the World Wide Web

#### 🌐 API 데이터 가져오기 흐름
1. API 문서 확인 → 사용 가능한 엔드포인트와 쿼리 파라미터 파악.

2. URL 구성 → 필요시 쿼리스트링(Query String) 추가.

3. Python에서 요청 보내기 (requests 활용).

4. 응답(JSON) → Python dict로 변환 → 데이터 처리.

#### 🧪 OMDb API 예제
- OMDb(Open Movie Database)는 영화 정보를 제공하는 API.
- API 문서에 따라 쿼리스트링 작성:
  `http://www.omdbapi.com/?t=Hackers&apikey=YOUR_API_KEY`
    - `t`: 제목으로 검색 (제목이 'Hacker'임을 찾는 쿼리스트링)
    - `apikey`: 발급받은 API 키 (필수)

<img width="595" height="418" alt="image" src="https://github.com/user-attachments/assets/934a3b60-1d39-4d9b-a066-4a5b297a4c43" />

```python
import requests

url = "http://www.omdbapi.com/"
params = {"t": "Hackers", "apikey": "YOUR_API_KEY"}

r = requests.get(url, params=params)
data = r.json()

for key, value in data.items():
    print(key, ":", value)
```

#### 📌 쿼리스트링(Query String)
- URL 뒤 `?`로 시작, `key=value` 형식.
- `&`로 여러 개의 파라미터 연결.
- 예: https://api.example.com/search?q=python&page=2

### 3. API 응답 다루기

#### request 의 `.json()` 메서드
  - 응답이 JSON 이면 자동 디코딩하여 `dict/list` 반환.
  - 파이썬 내장 `json.loads()`와 동일하지만, 응답 객체 전용.

```python
response = requests.get(url, params=params)
data = response.json()  # dict 반환
```

### 4. Library of Congress(LoC) API 예제
- 미국 의회도서관(Library of Congress)에서 제공하는 무료 API
- 검색 시 JSON 포맷으로 받으려면 `fo=json` 파라미터 추가.

```python
url = "https://www.loc.gov/search/"
params = {"q": "baseball", "fo": "json"}

r = requests.get(url, params=params)
data = r.json()

for item in data['results'][:5]:
    print(item['title'], "|", item.get('date'))
```

### 정리
| 개념           | 설명                                |
| ------------ | --------------------------------- |
| API          | 응용 프로그램 간 통신을 위한 인터페이스            |
| JSON         | API 데이터 전송의 표준 포맷                 |
| 쿼리스트링        | URL에 파라미터를 전달하는 방식 (`?key=value`) |
| requests.get | API 요청 전송                         |
| .json()      | JSON 응답을 Python dict/list로 변환     |


### 실무용 사용법 (꼭 읽기)
1. 인증키 관리
  - 키는 코드에 하드코딩하지 말고 환경변수/비밀관리(예: .env, CI/CD secret)로 관리.
  - 공용 저장소에 올리지 않도록 주의(특히 OMDb 등). [omdbapi](https://www.omdbapi.com/)
2. 에러/재시도
  - `response.raise_for_status()`로 HTTP 에러 감지.
  - 429(too many requests)나 5xx는 지수적 백오프로 재시도.
  - JSON 파싱 전 Content-Type 확인 또는 try: `r.json()` 예외 처리. 
3. 타임아웃 & 세션
  - `timeout` 필수, 대량 호출은 `requests.Session()`으로 커넥션 재사용.
  - 대용량 스트리밍은 `stream=True` 고려. 
4. 페이지네이션
  - OMDb: page 1–100, totalResults 참고. 
  - LoC: 검색 응답의 pagination 필드(있을 경우) 혹은 results 개수 기반으로 다음 요청 판단. (엔드포인트별 스펙은 공식 문서 확인) 
  - [The Library of Congress](https://www.loc.gov/apis/json-and-yaml/requests)
5. 레이트 리밋
  - LoC는 키 없이 사용 가능하지만 레이트 리밋 준수 권장. 배치/캐싱 전략을 함께 설계. [The Library of Congress](https://www.loc.gov/apis/json-and-yaml/working-within-limits/)
6. 파라미터는 params로
  - 수동 문자열 조합 대신 params 사용 → 인코딩/보안/가독성 향상. (쿼리스트링은 URL의 표준 구성 요소) [url.spec.whatwg.org](https://url.spec.whatwg.org/)

<hr>

## 

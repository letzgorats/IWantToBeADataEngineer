## 🐍 파이썬으로 데이터 불러오기 입문: Flat Files 완전 정복
데이터 분석의 시작은 데이터 불러오기(import)입니다. 이번 포스팅에서는 **Python으로 flat file(플랫 파일)** 을 불러오는 방법을 배웁니다. 
`.txt`, `.csv`와 같은 가장 기본적이면서도 강력한 파일 형식들을 다루며, Python의 기본 문법, NumPy, pandas를 통해 데이터를 읽는 실전 예제를 정리해볼게요.(본 내용은 DataCamp의 “Introduction to Importing Data in Python” 1장을 바탕으로 정리했습니다.)

### 📌 Flat File이란?
Flat file은 다음과 같은 특징을 가진 텍스트 기반 데이터 파일입니다.
- 행(row)은 하나의 레코드
- 열(column)은 속성 또는 변수
- 관계형 DB처럼 테이블 간 관계 없음
- 주로 `.csv`, `.tsv`, `.txt` 확장자를 가짐
- 예시
  ```python
  Name,Gender,Age,Survived
  Jack,male,23,1
  Rose,female,22,`
  ```

<img width="824" height="428" alt="image" src="https://github.com/user-attachments/assets/b4a157bb-b7b8-457b-b06c-d9be28f75425" />

##### Why we like flat files and the Zen of Python

> In PythonLand, there are currently hundreds of Python Enhancement Proposals, commonly referred to as PEPs. [PEP8](https://peps.python.org/pep-0008/),
> for example, is a standard style guide for Python, written by our sensei Guido van Rossum himself. It is the basis for how we here at DataCamp ask our instructors to style their code.
> Another one of my favorites is [PEP20](https://peps.python.org/pep-0020/), commonly called the Zen of Python. Its abstract is as follows:
  > Long time Pythoneer Tim Peters succinctly channels the BDFL's guiding principles for Python's design into 20 aphorisms, only 19 of which have been written down.
> If you don't know what the acronym `BDFL` stands for, I suggest that you look [here](https://docs.python.org/3.3/glossary.html#term-bdfl).
> You can print the Zen of Python in your shell by typing `import this` into it! You're going to do this now and the 5th aphorism (line) will say something of particular interest.
> The question you need to answer is: what is the 5th aphorism of the Zen of Python?

```python
import this

The Zen of Python, by Tim Peters

Beautiful is better than ugly.
Explicit is better than implicit.
Simple is better than complex.
Complex is better than complicated.
Flat is better than nested.
Sparse is better than dense.
Readability counts.
Special cases aren't special enough to break the rules.
Although practicality beats purity.
Errors should never pass silently.
Unless explicitly silenced.
In the face of ambiguity, refuse the temptation to guess.
There should be one-- and preferably only one --obvious way to do it.
Although that way may not be obvious at first unless you're Dutch.
Now is better than never.
Although never is often better than *right* now.
If the implementation is hard to explain, it's a bad idea.
If the implementation is easy to explain, it may be a good idea.
Namespaces are one honking great idea -- let's do more of those!
```

### 📁 1. 파이썬 내장 open() 함수로 텍스트 읽기
- 가장 기본적인 방법은 `open()` 함수를 사용하는 것입니다.
  ```python
  filename = "huck_finn.txt"
  file = open(filename, mode='r')  # 'r' is to read
  text = file.read()
  file.close()
  print(text)
  ```

#### 📌 Best Practice: with 문 (Context Manager)
```python
with open("huck_finn.txt", 'r') as file:   # 'file' 이라는 변수에 파일을 읽기모드로 받는다는 뜻!
    text = file.read()
print(text)
```
> ✅ `with` 문을 사용하면 파일을 자동으로 닫아줘서 더 안전하고 깔끔합니다.(close()를 할 필요가 없어서 간편하기도 하죠.)

### 🔢 2. NumPy로 숫자 기반 Flat File 읽기
`NumPy`는 수치형 데이터 처리를 위한 빠르고 효율적인 배열 라이브러리입니다.

#### ✅ 기본 예시
```python
import numpy as np

data = np.loadtxt("mnist_digits.txt", delimiter=',',skiprows=1,usecols=[0,2],dtype=str)
print(data)
```

#### 🎯 주요 옵션
- delimiter=',': 구분자 지정 (default: 공백)
- skiprows=1: 첫 행(헤더) 스킵 (파일의 맨 위에서부터 지정된 수만큼의 행을 무시하고 데이터를 읽기 시작)
- usecols=['cols','cols'], usecols=[0,2]: 일부 열만 선택 (필요한 열만 선택, 인덱스로 표현해도 가능)
- dtype=str: 문자열로 불러오기

```python
data = np.loadtxt("mnist_digits.txt", delimiter=',', skiprows=1, usecols=(0,2), dtype='str')
```
> ⚠️ 문자열/숫자가 섞인 경우엔 NumPy는 부적합합니다. 그럴 땐 **pandas** 로!

### 🐼 3. pandas로 CSV와 같은 Flat File 읽기
pandas는 테이블 형식 데이터를 다루는 최강의 파이썬 라이브러리입니다.

<img width="547" height="149" alt="image" src="https://github.com/user-attachments/assets/93e51c34-5328-4981-8750-5de3e37b6c0d" />

<img width="498" height="167" alt="image" src="https://github.com/user-attachments/assets/064f71d0-e714-473a-9818-6034ad70adb1" />

#### ✅ 가장 간단한 방법
```python
import pandas as pd

df = pd.read_csv("titanic.csv")
print(df.head())
```
- There are a number of arguments that pd.read_csv() takes 
  - `nrows` allows you to specify how many rows to read from the file.
    - For example, `nrows=10` will only import **the first 10 rows**.
  - `header` accepts row numbers to use as the column labels and marks the start of the data. If the file does not contain a header row, you can set `header=None`, and pandas will automatically assign integer column labels starting from 0 (e.g., 0, 1, 2, …).
  - `sep` sets the expected delimiter.
    - You can use ',' for comma-delimited.
    - You can use '\t' for tab-delimited.
  - `comment` takes characters that comments occur after in the file, indicating that any text starting with these characters should be ignored.
  - `na_values` takes a list of strings to identify as `NA/NaN`. By default, some values are already recognized as `NA/NaN`. Providing this argument will supply additional values.

#### ✅ pandas 의 read_csv 정리해보자
```python
pd.read_csv(FilePath, sep, header, names, index_col, skiprows, nrows, encoding, comment, chunksize)
```
- `FilePath` : 읽을 파일 Path, URL 도 가능
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file)
  ```
- `sep` : 구분자 입력, Default 는 쉼표(','), (공백의 경우 정규식 '\s+' 을 사용)
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file,sep='\s+')
  ```
- `header` : 파일의 데이터에 header가 없을 경우 `None` 입력, Default는 header 읽음
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file,sep='\s+',header=None)
  ```
- `names` : 파일의 데이터에 header가 없을 경우에 명시적으로 컬럼명 입력 가능(ex.
  ```python
  file = 'titanic_corrupt.txt'
  # 파일에 열 이름이 없고, 3개의 열이 있는 경우
  df = pd.read_csv(file, header=None, names=['col1', 'col2', 'col3'])
  
  # 파일에 열 이름이 있지만, 다른 이름으로 변경하려는 경우
  df = pd.read_csv(file, names=['new_col1', 'new_col2', 'new_col3'])
  ```
- `skiprows` : 입력한 개수 만큼 파일에서 행을 건너뛰고 읽음
  ```python
  file = 'titanic_corrupt.txt'
  # 첫 번째 행과 세 번째 행을 건너뛰고 데이터프레임으로 읽어오기
  data = pd.read_csv(file, skiprows=[0, 2])
  
  # 처음 2개의 행을 건너뛰고 데이터프레임으로 읽어오기
  data = pd.read_csv(file, skiprows=2)
  
  # 짝수 행을 읽고, 홀수 행을 건너뛰고 데이터프레임으로 읽어오기
  data = pd.read_csv(file, skiprows=lambda i: i % 2 != 0)
  ```
- `index_col` : CSV 파일의 특정 열을 데이터프레임의 인덱스로 지정할 때 사용 (ex.index_col='ID' 면 ID 컬럼이 인덱스 컬럼이 된다는 뜻!)
  ```python
  file = 'titanic_corrupt.txt'
  # 첫 번째 열을 인덱스로 설정하여 CSV 파일을 읽어옴
  data = pd.read_csv(file, index_col=0)
  
  # 'name' 열을 인덱스로 설정하여 CSV 파일을 읽어옴
  data = pd.read_csv(file, index_col='name')
  ```
- `nrows` : 파일 최상위에서부터 입력한 개수만큼의 데이터만 읽음
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file, nrows = 3)
  ```
- `encoding` : 파일의 문자 인코딩 타입 입력, 한글이면 'CP949'
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file, encoding = 'CP949')
  ```
- `comment` : 특정 문자는 주석으로 간주하고 읽지 않음 (ex. comment='/' 이면, '/'가 포함된 행은 주석으로 간주하고 읽지 않고 건너 뛴다!)
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file, comment='#')
  ```
- `chunksize` : 데이터를 한 번에 N행씩 나누어 읽을 수 있는 반복자(iterator) 를 반환함.
  - 반환되는 객체는 TextFileReader 이며, for 루프 또는 `next()`로 N개씩 데이터를 읽어들일 수 있다!
  - `TextFileReader`객체는 df 가 아니라 **generator** 다!
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file,sep='\t',comment='#',na_values=['Nothing'],chunksize = 4)  # data가 TextFileReader 객체(`data`는 chunk 단위의 `generator` 역할을 하는 객체)
  next(data)  # 다음 4행(chunk)을 DataFrame 형태로 반환
  next(data)
  next(data)
  next(data)

  ...OR
  
  for chunk in pd.read_csv(file, chunksize=4):
    print(chunk)  # 모든 청크를 순회하며 출력
  ```
  - next(data)는 매 호출 시 다음 블록(4행)을 반환하므로 누적은 하지 않음
  - 누적이 필요하다면 각 chunk를 리스트에 저장하거나 pd.concat() 등을 사용
  - 핵심은 next(data)는 chuncksize 만큼 행을 잘라서 보여준다.
  - `chunksize`는 한 번에 메모리에 다 올리기 어려운 **대용량 파일 처리에 유용**하다.
  - `TextFileReader`는 `lazy evaluation 방식`으로 작동하므로, **전체를 담는 DataFrame이 아니다!!**

#### 🌟 Pandas DataFrame의 장점
- 헤더 자동 인식
- 결측값 처리하는 방식
- 주석 처리하는 방식
- 문자열/숫자 혼합 데이터 처리 가능
- 다양한 파일 포맷 지원 (Excel, RDB, HDF5, MATLAB, JSON, SQL 등)

> 이러한 많은 이유로 pandas는 표준이자 best practice 가 되었습니다.

> 🧪 `pandas DataFrame`은 `NumPy` 배열로도 쉽게 변환 가능합니다!
```python
array = df.to_numpy()
```
- [DataFrame.values is not recommended. Use DataFrame.to_numpy() instead.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html)

### 📌 정리 : 어떤 방법을 써야 할까?
| 상황                 | 추천 방법                     |
| ------------------ | ------------------------- |
| 단순 텍스트 읽기          | `open()` or `with open()` |
| 숫자만 있는 대용량 데이터     | `np.loadtxt()`            |
| 문자열/숫자 혼합된 데이터     | `pd.read_csv()`           |
| 결측값, 주석, 특수 구분자 존재 | `pandas`가 가장 안정적          |

### 마무리
Flat File은 데이터 과학의 출발점입니다. NumPy는 빠른 수치 계산에, pandas는 유연한 데이터 처리에 강력합니다.

<hr>

## 📂 파이썬으로 다양한 파일 형식에서 데이터 불러오기 - 실무형 완벽 정리
데이터 과학자는 단순히 CSV만 다루지 않습니다. 업무 현장에서는 Excel, SAS, Stata, MATLAB, Pickle, HDF5 등 다양한 포맷의 데이터를 마주하게 되죠.
이번 포스트에서는 파이썬을 활용해 이런 다양한 파일들을 효율적으로 불러오는 방법을 소개합니다.(본 내용은 DataCamp의 "Introduction to Importing Data in Python" 챕터 2 내용을 정리한 것입니다.)

### 🥒 1. Pickled Files (피클 파일)
- **Pickle** 은 파이썬 객체를 **바이트 스트림(byte stream)** 으로 저장하는 **직렬화(serialization) 방식** 입니다.
- 리스트, 딕셔너리, 사용자 정의 객체 등 **복잡한 구조도 저장 가능** 합니다.
- 사람이 읽을 수는 없지만, **Python에서 바로 불러오기 좋음**

#### ✅ 불러오는 법
```python
import pickle

with open('data.pkl', 'rb') as file:
    data = pickle.load(file)
```

> `'rb'` = `read` + `binary`

> 피클은 JSON 처럼 읽을 수 없지만, 파이썬에서는 강력한 저장 형식입니다.

### 📊 2. Excel 파일 불러오기 (.xls, .xlsx)
- 거의 모든 직군이 다루는 파일
- pandas의 `read_excel()` 또는 `pd.ExcelFile()` 로 처리

#### ✅ 기본 사용법
```python
import pandas as pd

xls = pd.ExcelFile('data.xlsx')
print(xls.sheet_names)  # 시트 이름 확인

df1 = xls.parse('Sheet1')  # 시트 이름으로 로드
df2 = xls.parse(0) # 시트 인덱스로 로드
```

#### ✅ 커스터마이징 예시
```python
df = pd.read_excel('data.xlsx',
                   sheet_name='Sheet1',
                   skiprows=1,
                   usecols=['A', 'C'],
                   names=['ID', 'Value'])
```
> 실무에서는 시트마다 포맷이 다르므로 `sheet_name`, `skiprows`, `usecols` 옵션을 잘 조합하는 것이 중요합니다.

### 📈 3. SAS & Stata 파일

#### 파일 📘 SAS (.sas7bdat)
- `sas7bdat` 패키지를 사용해 읽음 (단점: 오래되었고, 유지 관리 중단됨)
```python
from sas7bdat import SAS7BDAT

with SAS7BDAT('urbanpop.sas7bdat') as file:
    df_sas = file.to_data_frame()
```

#### 📗 Stata (.dta)
- pandas가 직접 지원 (`read_stata()`)
```python
import pandas as pd

df_stata = pd.read_stata('data.dta')
```
> ✅ Stata는 경제/사회과학 계열에서 자주 쓰이며, .dta 파일을 그대로 DataFrame으로 가져올 수 있어 매우 편리합니다.

### 💾 4. HDF5 파일
- 대규모 수치형 데이터를 계층적(Hierarchical)으로 저장
- 천문학, 물리학 등에서 TB급 데이터 저장에 쓰임
- 예 : LIGO 중력파 프로젝트

#### ✅ 읽기 예시
```python
import h5py

file = h5py.File('LIGO_data.hdf5', 'r')
print(file.keys())  # ['meta', 'quality', 'strain']

meta = file['meta']
print(meta.keys())  # 메타 정보 키들

description = meta['Description']
print(description[:])  # numpy 배열로 반환
```

### 🧪 5. MATLAB 파일 (.mat)
- `Scipy` 라이브러리의 `loadmat()` 함수 사용
- MATLAB의 `.mat`은 변수들의 딕셔너리 저장소라고 생각하면 편함

#### ✅ 사용 예시

```python
from scipy.io import loadmat

mat = loadmat('workspace.mat')
print(mat.keys())  # MATLAB 변수 이름들

print(mat['x'])  # MATLAB의 변수 x 값 확인
```
> `.mat` 파일은 대부분 **Numpy 배열로 자동 변환** 됩니다. MATLAB → Python 이관 시 자주 쓰이는 방식입니다.

### ✅ 요약 테이블
| 파일 형식  | 확장자         | 불러오기 방법                                | 패키지        |
| ------ | ----------- | -------------------------------------- | ---------- |
| Pickle | `.pkl`      | `pickle.load()`                        | `pickle`   |
| Excel  | `.xlsx`     | `read_excel()` / `ExcelFile().parse()` | `pandas`   |
| SAS    | `.sas7bdat` | `SAS7BDAT().to_data_frame()`           | `sas7bdat` |
| Stata  | `.dta`      | `read_stata()`                         | `pandas`   |
| HDF5   | `.hdf5`     | `h5py.File()`                          | `h5py`     |
| MATLAB | `.mat`      | `loadmat()`                            | `scipy.io` |

### 마무리
현업 데이터 과학자라면, 다양한 파일 형식을 다루는 역량은 필수입니다. pandas와 SciPy, h5py 등은 그 핵심 도구입니다.



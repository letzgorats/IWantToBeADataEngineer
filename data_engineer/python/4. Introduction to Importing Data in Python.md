## 🐍 파이썬으로 데이터 불러오기 입문: Flat Files 완전 정복
데이터 분석의 시작은 데이터 불러오기(import)입니다. 이번 포스팅에서는 **Python으로 flat file(플랫 파일)** 을 불러오는 방법을 배웁니다. 
`.txt`, `.csv`와 같은 가장 기본적이면서도 강력한 파일 형식들을 다루며, Python의 기본 문법, NumPy, pandas를 통해 데이터를 읽는 실전 예제를 정리해볼게요.(본 내용은 DataCamp의 “Introduction to Importing Data in Python” 1장을 바탕으로 정리했습니다.)

### 📌 Flat File이란?
Flat file은 다음과 같은 특징을 가진 텍스트 기반 데이터 파일입니다.
- 행(row)은 하나의 레코드
- 열(column)은 속성 또는 변수
- 관계형 DB처럼 테이블 간 관계 없음
- 주로 `.csv`, `.tsv`, `.txt` 확장자를 가짐
- 예시
  ```python
  Name,Gender,Age,Survived
  Jack,male,23,1
  Rose,female,22,`
  ```

<img width="824" height="428" alt="image" src="https://github.com/user-attachments/assets/b4a157bb-b7b8-457b-b06c-d9be28f75425" />

##### Why we like flat files and the Zen of Python

> In PythonLand, there are currently hundreds of Python Enhancement Proposals, commonly referred to as PEPs. [PEP8](https://peps.python.org/pep-0008/),
> for example, is a standard style guide for Python, written by our sensei Guido van Rossum himself. It is the basis for how we here at DataCamp ask our instructors to style their code.
> Another one of my favorites is [PEP20](https://peps.python.org/pep-0020/), commonly called the Zen of Python. Its abstract is as follows:
  > Long time Pythoneer Tim Peters succinctly channels the BDFL's guiding principles for Python's design into 20 aphorisms, only 19 of which have been written down.
> If you don't know what the acronym `BDFL` stands for, I suggest that you look [here](https://docs.python.org/3.3/glossary.html#term-bdfl).
> You can print the Zen of Python in your shell by typing `import this` into it! You're going to do this now and the 5th aphorism (line) will say something of particular interest.
> The question you need to answer is: what is the 5th aphorism of the Zen of Python?

```python
import this

The Zen of Python, by Tim Peters

Beautiful is better than ugly.
Explicit is better than implicit.
Simple is better than complex.
Complex is better than complicated.
Flat is better than nested.
Sparse is better than dense.
Readability counts.
Special cases aren't special enough to break the rules.
Although practicality beats purity.
Errors should never pass silently.
Unless explicitly silenced.
In the face of ambiguity, refuse the temptation to guess.
There should be one-- and preferably only one --obvious way to do it.
Although that way may not be obvious at first unless you're Dutch.
Now is better than never.
Although never is often better than *right* now.
If the implementation is hard to explain, it's a bad idea.
If the implementation is easy to explain, it may be a good idea.
Namespaces are one honking great idea -- let's do more of those!
```

### 📁 1. 파이썬 내장 open() 함수로 텍스트 읽기
- 가장 기본적인 방법은 `open()` 함수를 사용하는 것입니다.
  ```python
  filename = "huck_finn.txt"
  file = open(filename, mode='r')  # 'r' is to read
  text = file.read()
  file.close()
  print(text)
  ```

#### 📌 Best Practice: with 문 (Context Manager)
```python
with open("huck_finn.txt", 'r') as file:   # 'file' 이라는 변수에 파일을 읽기모드로 받는다는 뜻!
    text = file.read()
print(text)
```
> ✅ `with` 문을 사용하면 파일을 자동으로 닫아줘서 더 안전하고 깔끔합니다.(close()를 할 필요가 없어서 간편하기도 하죠.)

### 🔢 2. NumPy로 숫자 기반 Flat File 읽기
`NumPy`는 수치형 데이터 처리를 위한 빠르고 효율적인 배열 라이브러리입니다.

#### ✅ 기본 예시
```python
import numpy as np

data = np.loadtxt("mnist_digits.txt", delimiter=',',skiprows=1,usecols=[0,2],dtype=str)
print(data)
```

#### 🎯 주요 옵션
- delimiter=',': 구분자 지정 (default: 공백)
- skiprows=1: 첫 행(헤더) 스킵 (파일의 맨 위에서부터 지정된 수만큼의 행을 무시하고 데이터를 읽기 시작)
- usecols=['cols','cols'], usecols=[0,2]: 일부 열만 선택 (필요한 열만 선택, 인덱스로 표현해도 가능)
- dtype=str: 문자열로 불러오기

```python
data = np.loadtxt("mnist_digits.txt", delimiter=',', skiprows=1, usecols=(0,2), dtype='str')
```
> ⚠️ 문자열/숫자가 섞인 경우엔 NumPy는 부적합합니다. 그럴 땐 **pandas** 로!

### 🐼 3. pandas로 CSV와 같은 Flat File 읽기
pandas는 테이블 형식 데이터를 다루는 최강의 파이썬 라이브러리입니다.

<img width="547" height="149" alt="image" src="https://github.com/user-attachments/assets/93e51c34-5328-4981-8750-5de3e37b6c0d" />

<img width="498" height="167" alt="image" src="https://github.com/user-attachments/assets/064f71d0-e714-473a-9818-6034ad70adb1" />

#### ✅ 가장 간단한 방법
```python
import pandas as pd

df = pd.read_csv("titanic.csv")
print(df.head())
```
- There are a number of arguments that pd.read_csv() takes 
  - `nrows` allows you to specify how many rows to read from the file.
    - For example, `nrows=10` will only import **the first 10 rows**.
  - `header` accepts row numbers to use as the column labels and marks the start of the data. If the file does not contain a header row, you can set `header=None`, and pandas will automatically assign integer column labels starting from 0 (e.g., 0, 1, 2, …).
  - `sep` sets the expected delimiter.
    - You can use ',' for comma-delimited.
    - You can use '\t' for tab-delimited.
  - `comment` takes characters that comments occur after in the file, indicating that any text starting with these characters should be ignored.
  - `na_values` takes a list of strings to identify as `NA/NaN`. By default, some values are already recognized as `NA/NaN`. Providing this argument will supply additional values.

#### ✅ pandas 의 read_csv 정리해보자
```python
pd.read_csv(FilePath, sep, header, names, index_col, skiprows, nrows, encoding, comment, chunksize)
```
- `FilePath` : 읽을 파일 Path, URL 도 가능
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file)
  ```
- `sep` : 구분자 입력, Default 는 쉼표(','), (공백의 경우 정규식 '\s+' 을 사용)
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file,sep='\s+')
  ```
- `header` : 파일의 데이터에 header가 없을 경우 `None` 입력, Default는 header 읽음
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file,sep='\s+',header=None)
  ```
- `names` : 파일의 데이터에 header가 없을 경우에 명시적으로 컬럼명 입력 가능(ex.
  ```python
  file = 'titanic_corrupt.txt'
  # 파일에 열 이름이 없고, 3개의 열이 있는 경우
  df = pd.read_csv(file, header=None, names=['col1', 'col2', 'col3'])
  
  # 파일에 열 이름이 있지만, 다른 이름으로 변경하려는 경우
  df = pd.read_csv(file, names=['new_col1', 'new_col2', 'new_col3'])
  ```
- `skiprows` : 입력한 개수 만큼 파일에서 행을 건너뛰고 읽음
  ```python
  file = 'titanic_corrupt.txt'
  # 첫 번째 행과 세 번째 행을 건너뛰고 데이터프레임으로 읽어오기
  data = pd.read_csv(file, skiprows=[0, 2])
  
  # 처음 2개의 행을 건너뛰고 데이터프레임으로 읽어오기
  data = pd.read_csv(file, skiprows=2)
  
  # 짝수 행을 읽고, 홀수 행을 건너뛰고 데이터프레임으로 읽어오기
  data = pd.read_csv(file, skiprows=lambda i: i % 2 != 0)
  ```
- `index_col` : CSV 파일의 특정 열을 데이터프레임의 인덱스로 지정할 때 사용 (ex.index_col='ID' 면 ID 컬럼이 인덱스 컬럼이 된다는 뜻!)
  ```python
  file = 'titanic_corrupt.txt'
  # 첫 번째 열을 인덱스로 설정하여 CSV 파일을 읽어옴
  data = pd.read_csv(file, index_col=0)
  
  # 'name' 열을 인덱스로 설정하여 CSV 파일을 읽어옴
  data = pd.read_csv(file, index_col='name')
  ```
- `nrows` : 파일 최상위에서부터 입력한 개수만큼의 데이터만 읽음
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file, nrows = 3)
  ```
- `encoding` : 파일의 문자 인코딩 타입 입력, 한글이면 'CP949'
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file, encoding = 'CP949')
  ```
- `comment` : 특정 문자는 주석으로 간주하고 읽지 않음 (ex. comment='/' 이면, '/'가 포함된 행은 주석으로 간주하고 읽지 않고 건너 뛴다!)
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file, comment='#')
  ```
- `chunksize` : 데이터를 한 번에 N행씩 나누어 읽을 수 있는 반복자(iterator) 를 반환함.
  - 반환되는 객체는 TextFileReader 이며, for 루프 또는 `next()`로 N개씩 데이터를 읽어들일 수 있다!
  - `TextFileReader`객체는 df 가 아니라 **generator** 다!
  ```python
  file = 'titanic_corrupt.txt'
  data = pd.read_csv(file,sep='\t',comment='#',na_values=['Nothing'],chunksize = 4)  # data가 TextFileReader 객체(`data`는 chunk 단위의 `generator` 역할을 하는 객체)
  next(data)  # 다음 4행(chunk)을 DataFrame 형태로 반환
  next(data)
  next(data)
  next(data)

  ...OR
  
  for chunk in pd.read_csv(file, chunksize=4):
    print(chunk)  # 모든 청크를 순회하며 출력
  ```
  - next(data)는 매 호출 시 다음 블록(4행)을 반환하므로 누적은 하지 않음
  - 누적이 필요하다면 각 chunk를 리스트에 저장하거나 pd.concat() 등을 사용
  - 핵심은 next(data)는 chuncksize 만큼 행을 잘라서 보여준다.
  - `chunksize`는 한 번에 메모리에 다 올리기 어려운 **대용량 파일 처리에 유용**하다.
  - `TextFileReader`는 `lazy evaluation 방식`으로 작동하므로, **전체를 담는 DataFrame이 아니다!!**

#### 🌟 Pandas DataFrame의 장점
- 헤더 자동 인식
- 결측값 처리하는 방식
- 주석 처리하는 방식
- 문자열/숫자 혼합 데이터 처리 가능
- 다양한 파일 포맷 지원 (Excel, RDB, HDF5, MATLAB, JSON, SQL 등)

> 이러한 많은 이유로 pandas는 표준이자 best practice 가 되었습니다.

> 🧪 `pandas DataFrame`은 `NumPy` 배열로도 쉽게 변환 가능합니다!
```python
array = df.to_numpy()
```
- [DataFrame.values is not recommended. Use DataFrame.to_numpy() instead.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html)

### 📌 정리 : 어떤 방법을 써야 할까?
| 상황                 | 추천 방법                     |
| ------------------ | ------------------------- |
| 단순 텍스트 읽기          | `open()` or `with open()` |
| 숫자만 있는 대용량 데이터     | `np.loadtxt()`            |
| 문자열/숫자 혼합된 데이터     | `pd.read_csv()`           |
| 결측값, 주석, 특수 구분자 존재 | `pandas`가 가장 안정적          |

### 마무리
Flat File은 데이터 과학의 출발점입니다. NumPy는 빠른 수치 계산에, pandas는 유연한 데이터 처리에 강력합니다.

<hr>

## 📂 파이썬으로 다양한 파일 형식에서 데이터 불러오기 - 실무형 완벽 정리
데이터 과학자는 단순히 CSV만 다루지 않습니다. 업무 현장에서는 Excel, SAS, Stata, MATLAB, Pickle, HDF5 등 다양한 포맷의 데이터를 마주하게 되죠.
이번 포스트에서는 파이썬을 활용해 이런 다양한 파일들을 효율적으로 불러오는 방법을 소개합니다.(본 내용은 DataCamp의 "Introduction to Importing Data in Python" 챕터 2 내용을 정리한 것입니다.)

### 🥒 1. Pickled Files (피클 파일)
- **Pickle** 은 파이썬 객체를 **바이트 스트림(byte stream)** 으로 저장하는 **직렬화(serialization) 방식** 입니다.
- 리스트, 딕셔너리, 사용자 정의 객체 등 **복잡한 구조도 저장 가능** 합니다.
- 사람이 읽을 수는 없지만, **Python에서 바로 불러오기 좋음**

#### ✅ 불러오는 법
```python
import pickle

with open('data.pkl', 'rb') as file:
    data = pickle.load(file)
```

> `'rb'` = `read` + `binary`

> 피클은 JSON 처럼 읽을 수 없지만, 파이썬에서는 강력한 저장 형식입니다.

### 📊 2. Excel 파일 불러오기 (.xls, .xlsx)
- 거의 모든 직군이 다루는 파일
- pandas의 `read_excel()` 또는 `pd.ExcelFile()` 로 처리

#### ✅ 기본 사용법
```python
import pandas as pd

xls = pd.ExcelFile('data.xlsx')
print(xls.sheet_names)  # 시트 이름 확인

df1 = xls.parse('Sheet1')  # 시트 이름으로 로드
df2 = xls.parse(0) # 시트 인덱스로 로드
```

#### ✅ 커스터마이징 예시
```python
df = pd.read_excel('data.xlsx',
                   sheet_name='Sheet1',
                   skiprows=1,
                   usecols=['A', 'C'],
                   names=['ID', 'Value'])
```
> 실무에서는 시트마다 포맷이 다르므로 `sheet_name`, `skiprows`, `usecols` 옵션을 잘 조합하는 것이 중요합니다.

### 📈 3. SAS & Stata 파일

#### 파일 📘 SAS (.sas7bdat) : 데이터 세트 파일 or (.sas7cat) : 카탈로그 파일
- `sas7bdat` 패키지를 사용해 읽음 (단점: 오래되었고, 유지 관리 중단됨)
```python
from sas7bdat import SAS7BDAT

with SAS7BDAT('urbanpop.sas7bdat') as file:
    df_sas = file.to_data_frame()
```
> 패키지 소문자(sas7bdat)에서 대문자(SAS7BDAT) 을 불러와야 합니다.

#### 📗 Stata (.dta)
- pandas가 직접 지원 (`read_stata()`)
- 이 경우에는 컨텍스트 관리자를 초기화할 필요조차 없습니다.(=with 구문 필요없습니다.)
```python
import pandas as pd

df_stata = pd.read_stata('data.dta')
```
> ✅ Stata는 경제/사회과학 계열에서 자주 쓰이며, .dta 파일을 그대로 DataFrame으로 가져올 수 있어 매우 편리합니다.

### 💾 4. HDF5 파일
- 대규모 수치형 데이터를 계층적(Hierarchical)으로 저장
- 천문학, 물리학 등에서 TB급 데이터 저장에 쓰임
- 예 : LIGO 중력파 프로젝트

#### ✅ 읽기 예시
```python
import h5py

file = h5py.File('LIGO_data.hdf5', 'r')  # 'r' is to read
print(file.keys())  # ['meta', 'quality', 'strain'] 이렇게 키에는 세 가지 핵심요소인, "meta", "quality", "strain" 이 있다.
```
> HDF5 파일에는 `meta`,`quality`, `strain` 키가 있으며, 이들은 각각 HDF 그룹입니다.

> 이러한 그룹을 "디렉토리"로 생각할 수 있습니다. LIGO 문서에서는 `meta` 에 파일의 메타 데이터가 포함되어 있다고 알려줍니다. `quality` 에는 데이터 품질에 대한 정보가 포함되어 있고,
> `strain` 에는 간섭계에서 얻은 데이터, LIGO가 수행한 주요 측정 데이터, 관심있는 데이터인 'strain' 데이터가 포함되어 있습니다.

> 이처럼, 각 그룹에 어떤 데이터와 메타 데이터가 있어야 하는지 알고 있다면, 손쉽게 접근할 수 있습니다. 하지만, 그렇지 않더라도 파일 구조의 계층적 특성 덕분에 쉽게 탐색할 수 있습니다.
```python
print(type(data['meta'])) # <class 'h5py._hl.group.Group'>
```

```python
meta = file['meta']
print(meta.keys())
# 메타 정보 키들 -> ['Description','DescriptionURL','Detector','Duration','GPSstart','Observatory','Type','UTCstart']

description = meta['Description']
print(np.array(description)) # numpy 배열로 반환(명시적으로 변환하는 법)
print(description[:])  # numpy 배열로 반환(자동변환하는 법)
# b'Strain data time series from LIGO' 가 출력된다.

print(np.array(description))
# b'H1' 가 출력된다.
```

### 🧪 5. MATLAB 파일 (.mat)
- MATLAB의 `.mat`은 변수들의 딕셔너리 저장소라고 생각하면 편함
- 파이썬의 `Scipy` 라이브러리의 `loadmat()` 함수를 사용함으로써 파이썬으로 가져올 수 있음

#### ✅ 사용 예시

```python
from scipy.io import loadmat

mat = loadmat('workspace.mat')
print(type(mat)) # <class 'dict'>

print(mat.keys())  # MATLAB 변수 이름들
print(mat['x'])  # MATLAB의 변수 x 값 확인
print(type(mat['x'])) # <class 'numpy.ndarry'>
```
> `.mat` 파일은 대부분 **Numpy 배열로 자동 변환** 됩니다. MATLAB → Python 이관 시 자주 쓰이는 방식입니다.

### ✅ 요약 테이블
| 파일 형식  | 확장자         | 불러오기 방법                                | 패키지        |
| ------ | ----------- | -------------------------------------- | ---------- |
| Pickle | `.pkl`      | `pickle.load()`                        | `pickle`   |
| Excel  | `.xlsx`     | `read_excel()` / `ExcelFile().parse()` | `pandas`   |
| SAS    | `.sas7bdat` | `SAS7BDAT().to_data_frame()`           | `sas7bdat` |
| Stata  | `.dta`      | `read_stata()`                         | `pandas`   |
| HDF5   | `.hdf5`     | `h5py.File()`                          | `h5py`     |
| MATLAB | `.mat`      | `loadmat()`                            | `scipy.io` |

### 마무리
현업 데이터 과학자라면, 다양한 파일 형식을 다루는 역량은 필수입니다. pandas와 SciPy, h5py 등은 그 핵심 도구입니다.

<hr>

## 🏛 파이썬에서 관계형 데이터베이스 다루기 - SQLAlchemy & pandas 활용법
실제 데이터 분석 프로젝트는 CSV 파일 하나만으로 끝나지 않습니다.
여러 개의 테이블이 연결된 관계형 데이터베이스(RDB) 를 다루는 능력은 데이터 엔지니어, 데이터 과학자에게 필수입니다.
이번 포스트에서는 파이썬을 사용해 SQL 데이터베이스에 연결하고, 데이터를 조회하고, 테이블 간 조인까지 해보는 관계형 데이터베이스 실전 입문 내용을 정리합니다.(본 글은 DataCamp의 "Introduction to Importing Data in Python" 3장 내용을 기반으로 작성되었습니다.)

### 🔗 1. 관계형 데이터베이스란?
- 데이터가 여러 테이블(table) 에 나뉘어 저장
- 테이블 간 관계를 통해 중복 없이 정규화된 데이터 관리
- 각 테이블은 엔티티(entity) 를 표현
- 각 행(row)은 하나의 레코드, 각 열(column)은 속성(attribute)

#### ✅ 예시: Northwind DB

<img width="1115" height="475" alt="image" src="https://github.com/user-attachments/assets/0ac6b188-01b1-47ba-aec3-953be5153573" />

- Orders 테이블 → 주문 정보
- Customers 테이블 → 고객 정보
- Employees 테이블 → 직원 정보
- 테이블은 기본키(Primary Key) 와 외래키(Foreign Key) 를 통해 연결됨

### 🛠 2. SQLAlchemy로 DB 연결하기

#### ✅ 데이터베이스 연결 단계
```python
from sqlalchemy import create_engine

engine = create_engine('sqlite:///Northwind.sqlite')  # SQLite 예시
```
- `create_engine()` 함수는 DB 에 접속할 수 있는 엔진을 생성합니다.
- 문자열 구성은 `'dialect+driver:///user:pass@host:port/dbname'` 형식입니다.

#### ✅ 테이블 이름 조회
```python
table_names = engine.table_names()  # 모든 테이블 이름 출력
print(table_names)
'''
['Categories', 'Customers', 'EmployeeTerritories','Employees','Order Details','Orders',
'Products','Region','Shippers','Suppliers','Territories']
'''
```

### 🧪 3. SQL 쿼리로 데이터 조회하기

#### ✅ 기본 워크플로우
```python
from sqlalchemy import create_engine
import pandas as pd

engine = create_engine('sqlite:///Northwind.sqlite')
con = engine.connect()

rs = con.execute("SELECT * FROM Orders")
df = pd.DataFrame(rs.fetchall())
df.columns = rs.keys()  # 컬럼명 설정 : df 컬럼을 rs의 컬럼과 같게 해야 같은 컬럼명을 df에서 볼 수 있다. (아니면 그냥 df의 컬럼명이 인덱스 형태로 되어버리니까!)
con.close()
```

#### ✅ with 문 (Context Manager) 사용 - close()할 필요가 없는 방식!!

```python
with engine.connect() as con:
    rs = con.execute("SELECT OrderID, OrderDate FROM Orders")
    df = pd.DataFrame(rs.fetchmany(size=5))
    df.columns = rs.keys()
```
> `fetchmany(size=n)` 은 `fetchall()`과 다르게 모든 행 대신 특정 행 개수를 가져옵니다.
 
### 🐼 4. pandas만으로 한 줄 SQL

```python
df = pd.read_sql_query("SELECT * FROM Customers", engine)
```
> 첫 번째 인수는 실행하려는 쿼리이고, 두 번째 인수는 연결하려는 엔진입니다.

- `read_sql_query()` 를 사용하면 쿼리 실행 + DataFrame 변환이 한 번에!
- 실무에서 가장 **간결하고 널리 쓰이는 방식**

### 🔀 5. 테이블 간 조인 (JOIN)
관계형 데이터베이스의 핵심은 테이블 간 연결입니다.

#### ✅ INNER JOIN 예시
```python
SELECT Orders.OrderID, Customers.CompanyName
FROM Orders
INNER JOIN Customers
ON Orders.CustomerID = Customers.CustomerID;
```
- `Orders` 테이블의 주문 ID와
- `Customers` 테이블의 회사 이름을
- `CustomerID` 를 기준으로 조인하여 가져옵니다.

#### 정리 : 실무를 위한 SQL+Python 기초
| 개념              | 사용 예시                 | 도구         |
| --------------- | --------------------- | ---------- |
| DB 연결           | `create_engine()`     | SQLAlchemy |
| 쿼리 실행           | `con.execute()`       | SQLAlchemy |
| 결과를 DataFrame으로 | `pd.DataFrame()`      | pandas     |
| 한 줄 SQL 조회      | `pd.read_sql_query()` | pandas     |
| 테이블 조인          | `INNER JOIN` 문        | SQL        |


### 마무리
> pandas + SQLAlchemy 조합은 관계형 데이터베이스를 효율적으로 다루는 파이썬의 실무 무기입니다.

다음 장에서는 웹에서 데이터를 직접 불러오거나 API를 통해 데이터를 다루는 방법을 배워볼 예정입니다.
RESTful API, JSON, 웹 스크래핑에 관심 있으시다면 계속 함께 공부해봅시다! 

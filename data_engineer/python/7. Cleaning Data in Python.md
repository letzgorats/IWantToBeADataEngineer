## 🧹 Python으로 배우는 데이터 클리닝 – Common Data Problems 완벽 가이드
데이터 분석이나 머신러닝 프로젝트에서 **데이터 클리닝(data cleaning)** 은 필수 과정입니다.
아무리 뛰어난 모델과 시각화 기법을 사용하더라도, 입력 데이터가 엉망이라면 결과 역시 믿을 수 없습니다.
이 글에서는 Python과 Pandas를 활용해 가장 흔하게 발생하는 데이터 문제와 해결 방법을 정리합니다.

### 1. 데이터 타입 제약 (Data Type Constraints)
#### 왜 중요한가?
데이터 타입이 잘못 지정되어 있으면 연산이 엉뚱하게 동작하거나, 분석 과정에서 오류가 발생할 수 있습니다.
예를 들어 숫자처럼 보이지만 문자열로 저장된 데이터는 합산 연산 시 숫자가 아닌 문자열이 이어붙여집니다.

#### 주요 예시

```python
import pandas as pd

# 잘못된 데이터 타입 예시
df = pd.DataFrame({
    "Revenue": ["$1000", "$2500", "$4000"]
})

print(df.dtypes)
# Revenue    object
```
> Revenue 컬럼이 object(문자열)로 저장되어 있어 합산 불가 → $ 기호 제거 후 숫자로 변환 필요.

#### 해결방법
```python
# 1) $ 제거
df["Revenue"] = df["Revenue"].str.strip('$')

# 2) 정수형 변환
df["Revenue"] = df["Revenue"].astype(int)

# 3) 검증
assert df["Revenue"].dtype == "int"
```

#### 💡 Tip:

> 금액에 소수점이 있다면 float로 변환합니다.

> 범주형 코드(예: 결혼 여부 0/1/2/3)는 category 타입으로 변환하면 불필요한 통계 계산을 방지할 수 있습니다.


### 1-1. 타입 진단 : `dtypes`, `info()`
```python
df.dtypes # 각 컬럼 dtype
df.info() # dtype -> 결측치 개요 확인
```
> 문자열 숫자, 날짜가 문자열(object)로 들어오는 경우가 가장 흔합니다.

### 1-2. 문자열 금액/숫자 → 숫자형 변환
현실 데이터는 `$`, `,`, `공백`, `음수 괄호(1,234)` 같은 **“잡음 문자”** 가 많습니다.
```python
import pandas as pd

df = pd.DataFrame({
    "revenue": ["$1,000", "2,500", " -300", "(450) ", "abc"]  # 다양하게 섞여 있음
})

# 1) 숫자/부호/소수점/하이픈만 남기고 모두 제거
df["revenue_clean"] = (
    df["revenue"].str.strip()
                   .str.replace(r"[^\d\-\.\(\)]", "", regex=True)  # 기호 외 제거
                   .str.replace(r"^\((.*)\)$", r"-\1", regex=True) # (450) → -450
)

# 2) 안전 변환: 잘못된 값은 NaN으로
df["revenue_num"] = pd.to_numeric(df["revenue_clean"], errors="coerce")

# 3) 검증
assert df["revenue_num"].dtype.kind in ("i", "f")  # 정수(i) 또는 실수(f)
```
#### 실무팁
> - "변환 실패를 강제로 에러 내고 싶다면 `errors="raise"` 를 사용해 조기에 데이터 문제를 드러내세요."

> - "통화 단위가 혼재(USD/JPY 등)하면 단위 표준화를 먼저 하세요!"

### 1-3. 범주형 코드 → 범주형(category)
숫자처럼 보이지만 “코드”인 경우(예: 결혼상태 0/1/2/3)는 범주형으로 변환합니다.
```python
from pandas.api.types import CategoricalDtype

marital_order = CategoricalDtype(categories=[0,1,2,3], ordered=True)
df["marital_status"] = df["marital_status"].astype(marital_order)

# 요약 통계가 평균/표준편차 대신, unique/최빈값 위주로 바뀜
df["marital_status"].describe()
```

#### 실무 팁
> - **"라벨 매핑** 까지 해두면 가독성과 EDA 효율이 올라갑니다."
```python
label = {0:"Never", 1:"Married", 2:"Separated", 3:"Divorced"}
df["marital_label"] = df["marital_status"].map(label)
```

### 1-4. 문자열 날짜 → `datetime64[ns]`
```python
import pandas as pd

df = pd.DataFrame({"subscription_date": ["2025/07/28", "07-29-2025", "2025.08.40", None]})

# 가장 안전한 방식: to_datetime 1단계 변환 후 벡터 연산 활용
df["subscription_dt"] = pd.to_datetime(
    df["subscription_date"],
    errors="coerce",        # 잘못된 날짜는 NaT
    infer_datetime_format=True, # 대체로 OK (버전에 따라 무시되기도 함)
)

# 형식이 일정하다면 format 지정이 가장 확실
df["subscription_dt"] = pd.to_datetime(df["subscription_date"], format="%Y/%m/%d", errors="coerce")
```
> 주의

> - "`errors="coerce"` 는 품질 경고등입니다. (분석 전, 반드시 결측/NaT 처리 기준을 수립해야 합니다.)"
> - "`dayfirst=True`(EU 형식) / `yearfirst=True` 로 지역 형식 혼선 해소합니다." 

### 1-5. 타임존 처리(있다면)
- naive(타임존 없음) → `tz_localize`
- aware(타임존 있음) → `tz_convert`

```python
# naive timestamp를 'Asia/Seoul' 기준으로 해석
df["ts_kr"] = pd.to_datetime(df["subscription_date"], errors="coerce").dt.tz_localize("Asia/Seoul")

# 이미 tz-aware이면 시간대 변환만
df["ts_utc"] = df["ts_kr"].dt.tz_convert("UTC")
```
> 실무 팁

> - "저장이 언제/어디 시간대인지" 명세를 고정하는게 좋습니다. 보통 원천 수집 시 UTC로 고정 저장되는데, 표시 단계에서 필요에 따라 지역 시간대로 변환하면 좋습니다.

### 2. 데이터 범위 제약 (Data Range Constraints)

#### 문제 상황
데이터 값이 허용 범위를 벗어나는 경우입니다.
- 예: 영화 평점(1~5)인데, 6점이 존재
- 예: 가입일이 현재 날짜보다 미래

#### 범위 검증 & 처리
```python
# 예: 영화 평점
movies = pd.DataFrame({"avg_rating": [4, 5, 6, 3]})

# 5점을 초과하는 값 찾기
invalid_rows = movies[movies["avg_rating"] > 5]

# 방법 1: 삭제
movies = movies[movies["avg_rating"] <= 5]

# 방법 2: 최대값으로 변경
movies.loc[movies["avg_rating"] > 5, "avg_rating"] = 5
```

#### 💡 Tip:

> 삭제는 데이터 손실을 유발하므로 비율이 적을 때만 사용.

> 날짜 처리 시 `pd.to_datetime()`으로 변환 후 비교:

```python
from datetime import date
df["subscription_date"] = pd.to_datetime(df["subscription_date"]).dt.date
today = date.today()
df = df[df["subscription_date"] <= today]
```

### 2-1. "미래 날짜" 필터링/보정
분석 기준일(오늘)과 비교하려면 **시점 통일**이 중요합니다.
```python
from datetime import date
import pandas as pd

# 1) Datetime 보존(권장): normalize()로 시각 제거(자정 기준)
today_ts = pd.Timestamp.today().normalize()      # 오늘 00:00:00 (로컬)
mask_future = df["subscription_dt"] > today_ts
future_rows = df[mask_future]

# 2) 바로 삭제
df_drop = df.loc[~mask_future].copy()

# 3) 하드 리밋(미래 → 오늘로 캡핑)
df_cap = df.copy()
df_cap.loc[mask_future, "subscription_dt"] = today_ts

# 4) 검증
assert (df_drop["subscription_dt"] <= today_ts).all()
assert (df_cap["subscription_dt"] <= today_ts).all()

```
> 왜 `dt.date` 대신 `datetime` 유지가 좋은가?

> - `datetime64[ns]` 는 벡터 연산/리샘플링/롤링 윈도우 등 시간 연산 최적화되어 있습니다.

> - 꼭 **날짜만 필요할 때만** `df["subscription_dt"].dt.date` 로 파생 컬럼을 만드는 것이 좋습니다.

### 2-2. 범위 제약 : 허용 구간 밖 값 다루기(삭제/캡핑/결측화)
영화 평점(1~5) 예시로 3가지 전형적 처리법을 비교해봅시다.
```python
movies = pd.DataFrame({"avg_rating": [1,3,5,6,0,4,7]})

# (A) 삭제 (outlier proportion이 매우 낮을 때)
mov_a = movies.loc[movies["avg_rating"].between(1,5)].copy()

# (B) 캡핑 (winsorize; 비즈니스 룰로 상한/하한 고정)
mov_b = movies.copy()
mov_b.loc[mov_b["avg_rating"] < 1, "avg_rating"] = 1
mov_b.loc[mov_b["avg_rating"] > 5, "avg_rating"] = 5

# (C) 결측화 후 적절한 Impute(다음 장에서 다룸)
mov_c = movies.copy()
mask = ~movies["avg_rating"].between(1,5)
mov_c.loc[mask, "avg_rating"] = pd.NA

# 검증
assert mov_a["avg_rating"].between(1,5).all()
assert mov_b["avg_rating"].between(1,5).all()
```

> 의사결정 가이드

> - 표본이 작으면 삭제는 위험합니다.
> - "평점 시스템이 1~5로 설계" 처럼 하드 룰이 명확하면 캡핑이 실용적입니다.
> - 데이터 생성 과정이 불확실하거나 비즈니스 룰이 모호하다면, 결측 처리 후 적절한 대치가 안전합니다.

### 3. 유니크 제약 (Uniqueness Constraints)

#### 문제 상황
중복 데이터가 존재하면 분석이 왜곡됩니다.
- **완전 중복** : 모든 컬럼 값이 동일한 행이 반복 (모든 컬럼 동일)
- **부분 중복** : 특정 키 컬럼은 같지만, 나머지 값이 다름 (일부 컬럼만 동일, 다른 값이 존재) - (갱신/오타/센서 변동 등)

#### 중복 찾기
```python
# 완전 중복 여부
df.duplicated()

# 특정 컬럼 기준
df.duplicated(subset=["first_name", "last_name"], keep=False)
```

#### 처리 방법
**1) 완전 중복 제거**
```python
df = df.drop_duplicates()
```

**2) 부분 중복 처리 (통계 기반 병합)**
```python
summaries = {"height": "max", "weight": "mean"}
df = df.groupby(["first_name", "last_name", "address"]).agg(summaries).reset_index()
```

#### 💡 Tip:

> 중복 처리를 무조건 삭제로 하지 말고, 도메인 지식에 따라 평균·최대·최소 등으로 병합.

> 중복 여부 확인 후 `assert df.duplicated().sum() == 0` 로 검증 가능.


### 3-1. `duplicated()`의 핵심 인자 - `subset`, `keep` 완전 이해
- `subset` : 중복 판단에 쓸 컬럼 집합 (기본값:`None` → 모든 컬럼 비교)
- `keep` : 어느 중복을 **원본(고유)**으로 보고 나머지를 `True`로 할 표시할지 결정** (**keep 동작에서 “남는 건 False, 중복으로 판단되는 건 True” 라는 점을 숙지하세요!**)
    - `first`(default) : **첫 번째**를 고유로 남기고 이후를 중복으로 표시
    - `last` : **마지막**을 고유로 남기고 이전을 중복으로 표시
    - `False` : **모든** 중복 발생 행을 `True` 로 표시 (즉, 고유로 남기지 않음)
    

예제 : 동명이인 + 주소 동일 시 같은 사람으로 판단
```python
people = pd.DataFrame({
    "first_name": ["Ann","Ann","Bob","Bob","Bob"],
    "last_name":  ["Kim","Kim","Lee","Lee","Lee"],
    "address":    ["Seoul","Seoul","Busan","Busan","Busan"],
    "height":     [160,160,175,176,175],
    "weight":     [55,55,70,72,71],
    "updated_at": ["2025-08-01","2025-08-01","2025-07-30","2025-08-01","2025-07-20"]
})

# 1) 완전 중복 탐지(모든 컬럼 동일해야 중복)
dup_full = people.duplicated(keep=False)

# 2) 키 기준 부분 중복 탐지(이름+주소만 기준)
key = ["first_name","last_name","address"]
dup_key_first = people.duplicated(subset=key, keep="first")
dup_key_last  = people.duplicated(subset=key, keep="last")
dup_key_all   = people.duplicated(subset=key, keep=False)

```
> `dup_key_first == True` : **첫 등장 이후**의 동일 키 행들

> `dup_key_last == True` : **마지막 이전**의 동일 키 행들

> `dup_key_all == True` : **동일 키의 모든 행** (첫/마지막 포함)

#### 데이터 원본
| idx | first\_name | last\_name | address | height | weight | updated\_at |
| --- | ----------- | ---------- | ------- | ------ | ------ | ----------- |
| 0   | Ann         | Kim        | Seoul   | 160    | 55     | 2025-08-01  |
| 1   | Ann         | Kim        | Seoul   | 160    | 55     | 2025-08-01  |
| 2   | Bob         | Lee        | Busan   | 175    | 70     | 2025-07-30  |
| 3   | Bob         | Lee        | Busan   | 176    | 72     | 2025-08-01  |
| 4   | Bob         | Lee        | Busan   | 175    | 71     | 2025-07-20  |

#### 1) `dup_full = people.duplicated(keep=False)` 
- 모든 컬럼이 동일해야 True
- `(0)` 과 `(1)`은 완전 동일  → 두 행 모두 True
- `(2)`,`(3)`,`(4)`는 height/weight가 달라서 완전 동일 아님 → False
- ✅ 결과
  ```python
  [ True, True, False, False, False ]
  ```

#### 2) `dup_key_first = people.duplicated(subset=key, keep="first")` : 위에서 아래로
- key = ["first_name","last_name","address"]
- 첫 번째 등장 행은 False, 이후 같은 키는 True.
- Ann Kim Seoul → idx 0은 False, idx 1은 True
- Bob Lee Busan → idx 2는 False, idx 3 True, idx 4 True
- ✅ 결과
  ```python
  [ False, True, False, True, True ]
  ```
  
#### 3) `dup_key_last = people.duplicated(subset=key, keep="last")` : 아래에서 위로
- 마지막 등장 행만 False, 그 이전 동일 키는 True.
- Ann Kim Seoul → idx 0 True, idx 1 False
- Bob Lee Busan → idx 2 True, idx 3 True, idx 4 False
- ✅ 결과
  ```python
  [ True, False, True, True, False ]
  ```
  
#### 4) `dup_key_all = people.duplicated(subset=key, keep=False)`
- 동일 키의 모든 행이 True (첫/마지막 구분 없음).
- Ann Kim Seoul → idx 0,1 모두 True
- Bob Lee Busan → idx 2,3,4 모두 True
- ✅ 결과:
  ```python
  [ True, True, True, True, True ]
  ```

### 📌 정리

> keep="first" → 첫 번째만 False, 나머지 True

> keep="last" → 마지막만 False, 나머지 True

> keep=False → 동일 키 전부 True

> **keep은 “True/False가 중복 여부”이지 “삭제할 행”과 1:1로 매칭되는 건 아님** (**`삭제`는 `drop_duplicates()`에서 실행됨)**


### 3-2. `drop_duplicates()` 도 같은 인자
`drop_duplicates(subset=...,keep=...)`는 위 로직을 그대로 "삭제"로 실행합니다.

#### 1) 완전 중복만 제거
```python
people_no_full_dup = people.drop_duplicates() # subset=None, keep="first"
```

#### 2) 키 기준, 최신만 남기고 이전 버전 삭제
실무 최빈 시나리오, `updated_at` 이 가장 최신인 1건만 남기기.
```python
# 1) 최신부터 정렬
people_sorted = people.sort_values("updated_at")  # 오래된→최신, 필요시 ascending=True/False 조정

# 2) 키 기준 마지막만 남기기 → 최신만 유지
people_latest = people_sorted.drop_duplicates(subset=key, keep="last")

# 검증: 키 기준으로 중복 없음
assert people_latest.duplicated(subset=key).sum() == 0
```

#### 3) 모든 중복 행만 따로 보고 싶다면
```python
only_dups = people[ people.duplicated(subset=key, keep=False) ].copy()
```
> 실무 팁

> - "최신만 남기기"는 정렬 기준이 중요합니다. 보통 `updated_at DESC` → keep="first"로도 구현합니다.
  ```python 
  people_latest = (
    people.sort_values("updated_at", ascending=False)
          .drop_duplicates(subset=key, keep="first")
   )
  ```

#### 4) "부분 중복" 병합(통계적 집계)
키는 같지만 수치가 다르면 **비즈니스 룰**로 결합합니다. (예 : 키는 `max`, 몸무게는 `mean`)
```python
agg_rule = {"height": "max", "weight": "mean"}
people_merged = (
    people.groupby(key, as_index=False)
          .agg(agg_rule)
)

# 검증
assert people_merged.duplicated(subset=key).sum() == 0
```
> 주의(SettingWithCopyWarning 회피)

> - "부분 업데이트는 항상 `.loc[조건,"col"] = 값` 형태로 하는게 좋습니다.
> - "중간 필터링 후 수정하려면, `.copy()` 로 명시적 복사본을 만들어 업데이트 하시면 됩니다.

### 마무리
데이터 클리닝의 핵심은 **데이터 타입, 값의 범위, 중복 여부** 를 철저히 점검하고, 상황에 맞는 처리 방식을 선택하는 것입니다.

| 문제 유형  | 주요 원인                  | 처리 방법 예시                                |
| ------ | ---------------------- | --------------------------------------- |
| 타입 오류  | CSV 파싱, 잘못된 입력, 문자열 숫자 | `.astype()`, `.to_datetime()`           |
| 범위 초과  | 입력 실수, 시스템 오류          | 필터링, 하드 리밋, NaN 처리                      |
| 중복 데이터 | 데이터 병합, 입력 중복          | `.drop_duplicates()`, `groupby().agg()` |


<hr>

## 📝 Python으로 카테고리 & 텍스트 데이터 클리닝 완벽 가이드

### 1. Membership Contraints (카테고리 값 유효성 제약)

<img width="1193" height="280" alt="image" src="https://github.com/user-attachments/assets/f66d728e-ac21-4f46-846c-008d1be93c74" />

#### 1-1. 문제 개념
- **카테고리 데이터** : 결혼 상태, 혈액형, 대출 상태 등 → 사전에 정의된 유한한 값만 허용됨
- 문제 발생 원인:
    - 자유 입력(Free text) vs 드롭다운 불일치
    - 파싱 오류
    - 잘못된 값 입력
  
<img width="1046" height="350" alt="image" src="https://github.com/user-attachments/assets/0692a165-b2b1-4c89-9531-7afe8a44d6d5" />


#### 1-2. 유효성 검증 & 잘못된 값 찾기

```python
import pandas as pd

# 예시 데이터
study_data = pd.DataFrame({
    "name": ["Alice", "Bob", "Chris"],
    "blood_type": ["A+", "B-", "Z+"]  # Z+는 잘못된 값
})

categories = pd.DataFrame({
    "blood_type": ["A+", "A-", "B+", "B-", "AB+", "AB-", "O+", "O-"]
})

# 허용되지 않는 값 찾기
invalid_values = set(study_data["blood_type"]) - set(categories["blood_type"])
# invalid_values = set(study_data["blood_type"]).difference(categories["blood_type"])
print(invalid_values)  # {'Z+'}

# 잘못된 행 필터링 (anti join 효과)
invalid_rows = study_data[study_data["blood_type"].isin(invalid_values)]

# 유효한 값만 남기기
valid_rows = study_data[~study_data["blood_type"].isin(invalid_values)]
```

> 💡 Tip

> 카테고리 값 목록은 별도 테이블/리스트로 관리하면 유지보수가 쉽숩니다.

> `isin`은 Membership Check 핵심 함수!

### 2. Categorical Variables(카테고리 값 표준화)

#### 2-1. 대소문자/공백 문제

```python
demographics = pd.DataFrame({"marriage_status": ["Married", "unmarried", " married ", "UNMARRIED"]})

# Get marriage status column
marriage_status = demographics['marriage_status']
marriage_status.value_counts()

'''
unmarried 352
married   268
MARRIED   204
UNMARRIED 176
dtype : int64
'''
```
> `.value_counts()` 메서드는 Series 에만 작동합니다.

```python
# Get value counts on DataFrame
m = demographics.groupby('marriage_status')
m.count()
```
> DataFrame 경우, 열을 기준으로 그룹화하고 `.count()` 메서드를 사용할 수 있습니다.


- **대/소문자 변환**


```python
# 대/소문자 변환
marriage_status["marriage_status"] = marraige_status["marriage_status"].str.upper()
marriage_status["marriage_status"].value_counts()

marriage_status["marriage_status"] = marriage_status["marriage_status"].str.lower()
marriage_status["marriage_status"].value_counts()

'''
UNMARRIED  528
MARRIED  472
'''
```

- **Trailing spaces** : `married `,`married`,`unmarried`,` unmarried `,...

```python
# 앞뒤 공백 제거
marriage_status = demographics["marriage_status"]
marriage_status.value_counts()

marriage_status["marriage_status"] = marriage_status["marriage_status"].str.strip()
'''
unmarried 352
married   268
MARRIED   204
UNMARRIED 176
dtype : int64
'''

# Strip all spaces
demographics = demographics["marriage_status"].str.strip()
demographics["marriage_status"].value_counts()

'''
unmarried 528
married   472
'''
```

#### 2-2. 구간별 범주화(`cut` vs `qcut`)

- **`qcut`**: 분위수 기반 구간 -> `상대평가` 라고 생각하면 쉽습니다!
```python
import numpy as np
import pandas as pd

group_names = ['0-200K','200K-500K','500K+']
demographics['income_group'] = pd.qcut(demographics['household_income'], q=3, labels = group_names)

# print income_group column
demographics[['income_group', 'household_income']]

'''
         category    household_income
0      200K-500K      189243
1          500K+      778533
'''
```

- **`cut`** : 고정 구간 -> `절대평가` 라고 생각하면 쉽습니다!
```python
# Using cut() - create category ranges and names

ranges = [0,200000,500000,np.inf]
group_names = ['0-200K','200K-500K','500K+']

# Create income group column
demographics['income_group'] = pd.cut(demographics['household_income'], bins=ranges, labels = group_names)
demographics[['income_group', 'household_income']]

'''
         category    household_income
0         0-200K      189243
1          500K+      778533
'''
```

> `cut` → 규칙/기준 확실할 때 : 절대평가

<img width="969" height="336" alt="image" src="https://github.com/user-attachments/assets/20756f8c-f8a3-4728-9568-bcf925c599c1" />

> `qcut` → 데이터 분포에 맞춰 균등 개수로 나눌 때 : 상대평가

<img width="960" height="458" alt="image" src="https://github.com/user-attachments/assets/25689dc4-8fc9-4e59-94be-dc37a658e62c" />

#### 2-3. 범주 축소(카테고리 매핑)

```python
os_map = {
    "Windows": "DesktopOS",
    "MacOS": "DesktopOS",
    "Linux": "DesktopOS",
    "Android": "MobileOS",
    "iOS": "MobileOS"
}

df["os_type"] = df["os_name"].replace(os_map)
df["os_type"].unique()

'''
array(['DesktopOS','MobileOs'],dtype=object)
'''
```

> 'DektopOs' 와 'MobileOs' 카테고리로 2개로 축소하고 싶다면, mapping을 활용하면 됩니다. 키-값 형태의 매핑을 활용해서, "os_type" 이라는 컬럼에 각 카테고리별로 축소-생성할 수 있습니다.


- (ex)
```python
# Create ranges for categories
label_ranges = [0, 60, 180, np.inf]
label_names = ['short', 'medium', 'long']

# Create wait_type column
airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, 
                                labels = label_names)

# Create mappings and replace
mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', 
            'Thursday': 'weekday', 'Friday': 'weekday', 
            'Saturday': 'weekend', 'Sunday': 'weekend'}

airlines['day_week'] = airlines['day'].replace(mappings)
```
<img width="1187" height="276" alt="image" src="https://github.com/user-attachments/assets/4039158d-09fe-491f-87b6-8ec015499657" />


### 3. Cleaning Text Data(문자열 정리)

#### 3-1. 기본 문자열 치환

```python
import numpy as np

phones = pd.DataFrame({
    "full_name": ["Alice Smith", "Bob Lee", "Chris Kim"],
    "phone": ["+8210-1234-5678", "0044-20-1234-5678", "1234"]
})

# '+' → '00'
phones["phone"] = phones["phone"].str.replace("+", "00", regex=False)

# '-' 제거
phones["phone"] = phones["phone"].str.replace("-", "", regex=False)

# 길이 < 10 → NaN
phones.loc[phones["phone"].str.len() < 10, "phone"] = np.nan
```

#### 3-2. 정규표현식(Regex)으로 숫자만 추출

```python
# 숫자가 아닌 모든 문자 제거
phones["phone_digits"] = phones["phone"].str.replace(r"\D", "", regex=True)
```
> `\D` = 숫자가 아닌 문자

> `\d` = 숫자

> `regex=True` 옵션 필수

#### 3-3. 데이터 검증(Assert)

```python
# 최소 길이 10 이상 확인
assert phones["phone_digits"].str.len().min() >= 10

# '+' 또는 '-' 존재 여부 확인
assert not phones["phone_digits"].str.contains(r"[\+\-]", regex=True).any()
```

### 4. 실무 체크리스트

| 문제 유형       | 주요 원인                  | 처리 방법 예시                       |
| ----------- | ---------------------- | ------------------------------ |
| 잘못된 카테고리 값  | 입력 오류, 파싱 실패           | `isin` / anti join / 매핑        |
| 대소문자·공백 불일치 | 수동 입력, 시스템 병합 시 포맷 깨짐  | `.str.lower()`, `.str.strip()` |
| 카테고리 과다     | 디바이스, 제품군 등 세분화 지나침    | 매핑(통합)                         |
| 문자열 형식 불일치  | 전화번호, 우편번호, ID 등 규격 깨짐 | `.str.replace()`, Regex        |


### 📌 마무리

- **Membership Check**로 유효한 값만 허용
- **포맷 표준화**(대소문자, 공백, 기호)로 집계 정확도 향상
- **범주화 & 범주 축소**로 분석 가독성 및 모델 안정성 확보
- **정규표현식**으로 패턴 기반 문자열 정리
- **Assert** 문으로 품질 불변식 유지










































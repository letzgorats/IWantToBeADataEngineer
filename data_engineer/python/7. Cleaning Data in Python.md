## 🧹 Python으로 배우는 데이터 클리닝 – Common Data Problems 완벽 가이드
데이터 분석이나 머신러닝 프로젝트에서 **데이터 클리닝(data cleaning)** 은 필수 과정입니다.
아무리 뛰어난 모델과 시각화 기법을 사용하더라도, 입력 데이터가 엉망이라면 결과 역시 믿을 수 없습니다.
이 글에서는 Python과 Pandas를 활용해 가장 흔하게 발생하는 데이터 문제와 해결 방법을 정리합니다.

### 1. 데이터 타입 제약 (Data Type Constraints)
#### 왜 중요한가?
데이터 타입이 잘못 지정되어 있으면 연산이 엉뚱하게 동작하거나, 분석 과정에서 오류가 발생할 수 있습니다.
예를 들어 숫자처럼 보이지만 문자열로 저장된 데이터는 합산 연산 시 숫자가 아닌 문자열이 이어붙여집니다.

#### 주요 예시

```python
import pandas as pd

# 잘못된 데이터 타입 예시
df = pd.DataFrame({
    "Revenue": ["$1000", "$2500", "$4000"]
})

print(df.dtypes)
# Revenue    object
```
> Revenue 컬럼이 object(문자열)로 저장되어 있어 합산 불가 → $ 기호 제거 후 숫자로 변환 필요.

#### 해결방법
```python
# 1) $ 제거
df["Revenue"] = df["Revenue"].str.strip('$')

# 2) 정수형 변환
df["Revenue"] = df["Revenue"].astype(int)

# 3) 검증
assert df["Revenue"].dtype == "int"
```

#### 💡 Tip:

> 금액에 소수점이 있다면 float로 변환합니다.

> 범주형 코드(예: 결혼 여부 0/1/2/3)는 category 타입으로 변환하면 불필요한 통계 계산을 방지할 수 있습니다.


### 1-1. 타입 진단 : `dtypes`, `info()`
```python
df.dtypes # 각 컬럼 dtype
df.info() # dtype -> 결측치 개요 확인
```
> 문자열 숫자, 날짜가 문자열(object)로 들어오는 경우가 가장 흔합니다.

### 1-2. 문자열 금액/숫자 → 숫자형 변환
현실 데이터는 `$`, `,`, `공백`, `음수 괄호(1,234)` 같은 **“잡음 문자”** 가 많습니다.
```python
import pandas as pd

df = pd.DataFrame({
    "revenue": ["$1,000", "2,500", " -300", "(450) ", "abc"]  # 다양하게 섞여 있음
})

# 1) 숫자/부호/소수점/하이픈만 남기고 모두 제거
df["revenue_clean"] = (
    df["revenue"].str.strip()
                   .str.replace(r"[^\d\-\.\(\)]", "", regex=True)  # 기호 외 제거
                   .str.replace(r"^\((.*)\)$", r"-\1", regex=True) # (450) → -450
)

# 2) 안전 변환: 잘못된 값은 NaN으로
df["revenue_num"] = pd.to_numeric(df["revenue_clean"], errors="coerce")

# 3) 검증
assert df["revenue_num"].dtype.kind in ("i", "f")  # 정수(i) 또는 실수(f)
```
#### 실무팁
> - "변환 실패를 강제로 에러 내고 싶다면 `errors="raise"` 를 사용해 조기에 데이터 문제를 드러내세요."

> - "통화 단위가 혼재(USD/JPY 등)하면 단위 표준화를 먼저 하세요!"

### 1-3. 범주형 코드 → 범주형(category)
숫자처럼 보이지만 “코드”인 경우(예: 결혼상태 0/1/2/3)는 범주형으로 변환합니다.
```python
from pandas.api.types import CategoricalDtype

marital_order = CategoricalDtype(categories=[0,1,2,3], ordered=True)
df["marital_status"] = df["marital_status"].astype(marital_order)

# 요약 통계가 평균/표준편차 대신, unique/최빈값 위주로 바뀜
df["marital_status"].describe()
```

#### 실무 팁
> - **"라벨 매핑** 까지 해두면 가독성과 EDA 효율이 올라갑니다."
```python
label = {0:"Never", 1:"Married", 2:"Separated", 3:"Divorced"}
df["marital_label"] = df["marital_status"].map(label)
```

### 1-4. 문자열 날짜 → `datetime64[ns]`
```python
import pandas as pd

df = pd.DataFrame({"subscription_date": ["2025/07/28", "07-29-2025", "2025.08.40", None]})

# 가장 안전한 방식: to_datetime 1단계 변환 후 벡터 연산 활용
df["subscription_dt"] = pd.to_datetime(
    df["subscription_date"],
    errors="coerce",        # 잘못된 날짜는 NaT
    infer_datetime_format=True, # 대체로 OK (버전에 따라 무시되기도 함)
)

# 형식이 일정하다면 format 지정이 가장 확실
df["subscription_dt"] = pd.to_datetime(df["subscription_date"], format="%Y/%m/%d", errors="coerce")
```
> 주의

> - "`errors="coerce"` 는 품질 경고등입니다. (분석 전, 반드시 결측/NaT 처리 기준을 수립해야 합니다.)"
> - "`dayfirst=True`(EU 형식) / `yearfirst=True` 로 지역 형식 혼선 해소합니다." 

### 1-5. 타임존 처리(있다면)
- naive(타임존 없음) → `tz_localize`
- aware(타임존 있음) → `tz_convert`

```python
# naive timestamp를 'Asia/Seoul' 기준으로 해석
df["ts_kr"] = pd.to_datetime(df["subscription_date"], errors="coerce").dt.tz_localize("Asia/Seoul")

# 이미 tz-aware이면 시간대 변환만
df["ts_utc"] = df["ts_kr"].dt.tz_convert("UTC")
```
> 실무 팁

> - "저장이 언제/어디 시간대인지" 명세를 고정하는게 좋습니다. 보통 원천 수집 시 UTC로 고정 저장되는데, 표시 단계에서 필요에 따라 지역 시간대로 변환하면 좋습니다.

### 2. 데이터 범위 제약 (Data Range Constraints)

#### 문제 상황
데이터 값이 허용 범위를 벗어나는 경우입니다.
- 예: 영화 평점(1~5)인데, 6점이 존재
- 예: 가입일이 현재 날짜보다 미래

#### 범위 검증 & 처리
```python
# 예: 영화 평점
movies = pd.DataFrame({"avg_rating": [4, 5, 6, 3]})

# 5점을 초과하는 값 찾기
invalid_rows = movies[movies["avg_rating"] > 5]

# 방법 1: 삭제
movies = movies[movies["avg_rating"] <= 5]

# 방법 2: 최대값으로 변경
movies.loc[movies["avg_rating"] > 5, "avg_rating"] = 5
```

#### 💡 Tip:

> 삭제는 데이터 손실을 유발하므로 비율이 적을 때만 사용.

> 날짜 처리 시 `pd.to_datetime()`으로 변환 후 비교:

```python
from datetime import date
df["subscription_date"] = pd.to_datetime(df["subscription_date"]).dt.date
today = date.today()
df = df[df["subscription_date"] <= today]
```

### 2-1. "미래 날짜" 필터링/보정
분석 기준일(오늘)과 비교하려면 **시점 통일**이 중요합니다.
```python
from datetime import date
import pandas as pd

# 1) Datetime 보존(권장): normalize()로 시각 제거(자정 기준)
today_ts = pd.Timestamp.today().normalize()      # 오늘 00:00:00 (로컬)
mask_future = df["subscription_dt"] > today_ts
future_rows = df[mask_future]

# 2) 바로 삭제
df_drop = df.loc[~mask_future].copy()

# 3) 하드 리밋(미래 → 오늘로 캡핑)
df_cap = df.copy()
df_cap.loc[mask_future, "subscription_dt"] = today_ts

# 4) 검증
assert (df_drop["subscription_dt"] <= today_ts).all()
assert (df_cap["subscription_dt"] <= today_ts).all()

```
> 왜 `dt.date` 대신 `datetime` 유지가 좋은가?

> - `datetime64[ns]` 는 벡터 연산/리샘플링/롤링 윈도우 등 시간 연산 최적화되어 있습니다.

> - 꼭 **날짜만 필요할 때만** `df["subscription_dt"].dt.date` 로 파생 컬럼을 만드는 것이 좋습니다.

### 2-2. 범위 제약 : 허용 구간 밖 값 다루기(삭제/캡핑/결측화)
영화 평점(1~5) 예시로 3가지 전형적 처리법을 비교해봅시다.
```python
movies = pd.DataFrame({"avg_rating": [1,3,5,6,0,4,7]})

# (A) 삭제 (outlier proportion이 매우 낮을 때)
mov_a = movies.loc[movies["avg_rating"].between(1,5)].copy()

# (B) 캡핑 (winsorize; 비즈니스 룰로 상한/하한 고정)
mov_b = movies.copy()
mov_b.loc[mov_b["avg_rating"] < 1, "avg_rating"] = 1
mov_b.loc[mov_b["avg_rating"] > 5, "avg_rating"] = 5

# (C) 결측화 후 적절한 Impute(다음 장에서 다룸)
mov_c = movies.copy()
mask = ~movies["avg_rating"].between(1,5)
mov_c.loc[mask, "avg_rating"] = pd.NA

# 검증
assert mov_a["avg_rating"].between(1,5).all()
assert mov_b["avg_rating"].between(1,5).all()
```

> 의사결정 가이드

> - 표본이 작으면 삭제는 위험합니다.
> - "평점 시스템이 1~5로 설계" 처럼 하드 룰이 명확하면 캡핑이 실용적입니다.
> - 데이터 생성 과정이 불확실하거나 비즈니스 룰이 모호하다면, 결측 처리 후 적절한 대치가 안전합니다.

### 3. 유니크 제약 (Uniqueness Constraints)

#### 문제 상황
중복 데이터가 존재하면 분석이 왜곡됩니다.
- **완전 중복** : 모든 컬럼 값이 동일한 행이 반복 (모든 컬럼 동일)
- **부분 중복** : 특정 키 컬럼은 같지만, 나머지 값이 다름 (일부 컬럼만 동일, 다른 값이 존재) - (갱신/오타/센서 변동 등)

#### 중복 찾기
```python
# 완전 중복 여부
df.duplicated()

# 특정 컬럼 기준
df.duplicated(subset=["first_name", "last_name"], keep=False)
```

#### 처리 방법
**1) 완전 중복 제거**
```python
df = df.drop_duplicates()
```

**2) 부분 중복 처리 (통계 기반 병합)**
```python
summaries = {"height": "max", "weight": "mean"}
df = df.groupby(["first_name", "last_name", "address"]).agg(summaries).reset_index()
```

#### 💡 Tip:

> 중복 처리를 무조건 삭제로 하지 말고, 도메인 지식에 따라 평균·최대·최소 등으로 병합.

> 중복 여부 확인 후 `assert df.duplicated().sum() == 0` 로 검증 가능.


### 3-1. `duplicated()`의 핵심 인자 - `subset`, `keep` 완전 이해
- `subset` : 중복 판단에 쓸 컬럼 집합 (기본값:`None` → 모든 컬럼 비교)
- `keep` : 어느 중복을 **원본(고유)**으로 보고 나머지를 `True`로 할 표시할지 결정** (**keep 동작에서 “남는 건 False, 중복으로 판단되는 건 True” 라는 점을 숙지하세요!**)
    - `first`(default) : **첫 번째**를 고유로 남기고 이후를 중복으로 표시
    - `last` : **마지막**을 고유로 남기고 이전을 중복으로 표시
    - `False` : **모든** 중복 발생 행을 `True` 로 표시 (즉, 고유로 남기지 않음)
    

예제 : 동명이인 + 주소 동일 시 같은 사람으로 판단
```python
people = pd.DataFrame({
    "first_name": ["Ann","Ann","Bob","Bob","Bob"],
    "last_name":  ["Kim","Kim","Lee","Lee","Lee"],
    "address":    ["Seoul","Seoul","Busan","Busan","Busan"],
    "height":     [160,160,175,176,175],
    "weight":     [55,55,70,72,71],
    "updated_at": ["2025-08-01","2025-08-01","2025-07-30","2025-08-01","2025-07-20"]
})

# 1) 완전 중복 탐지(모든 컬럼 동일해야 중복)
dup_full = people.duplicated(keep=False)

# 2) 키 기준 부분 중복 탐지(이름+주소만 기준)
key = ["first_name","last_name","address"]
dup_key_first = people.duplicated(subset=key, keep="first")
dup_key_last  = people.duplicated(subset=key, keep="last")
dup_key_all   = people.duplicated(subset=key, keep=False)

```
> `dup_key_first == True` : **첫 등장 이후**의 동일 키 행들

> `dup_key_last == True` : **마지막 이전**의 동일 키 행들

> `dup_key_all == True` : **동일 키의 모든 행** (첫/마지막 포함)

#### 데이터 원본
| idx | first\_name | last\_name | address | height | weight | updated\_at |
| --- | ----------- | ---------- | ------- | ------ | ------ | ----------- |
| 0   | Ann         | Kim        | Seoul   | 160    | 55     | 2025-08-01  |
| 1   | Ann         | Kim        | Seoul   | 160    | 55     | 2025-08-01  |
| 2   | Bob         | Lee        | Busan   | 175    | 70     | 2025-07-30  |
| 3   | Bob         | Lee        | Busan   | 176    | 72     | 2025-08-01  |
| 4   | Bob         | Lee        | Busan   | 175    | 71     | 2025-07-20  |

#### 1) `dup_full = people.duplicated(keep=False)` 
- 모든 컬럼이 동일해야 True
- `(0)` 과 `(1)`은 완전 동일  → 두 행 모두 True
- `(2)`,`(3)`,`(4)`는 height/weight가 달라서 완전 동일 아님 → False
- ✅ 결과
  ```python
  [ True, True, False, False, False ]
  ```

#### 2) `dup_key_first = people.duplicated(subset=key, keep="first")` : 위에서 아래로
- key = ["first_name","last_name","address"]
- 첫 번째 등장 행은 False, 이후 같은 키는 True.
- Ann Kim Seoul → idx 0은 False, idx 1은 True
- Bob Lee Busan → idx 2는 False, idx 3 True, idx 4 True
- ✅ 결과
  ```python
  [ False, True, False, True, True ]
  ```
  
#### 3) `dup_key_last = people.duplicated(subset=key, keep="last")` : 아래에서 위로
- 마지막 등장 행만 False, 그 이전 동일 키는 True.
- Ann Kim Seoul → idx 0 True, idx 1 False
- Bob Lee Busan → idx 2 True, idx 3 True, idx 4 False
- ✅ 결과
  ```python
  [ True, False, True, True, False ]
  ```
  
#### 4) `dup_key_all = people.duplicated(subset=key, keep=False)`
- 동일 키의 모든 행이 True (첫/마지막 구분 없음).
- Ann Kim Seoul → idx 0,1 모두 True
- Bob Lee Busan → idx 2,3,4 모두 True
- ✅ 결과:
  ```python
  [ True, True, True, True, True ]
  ```

### 📌 정리

> keep="first" → 첫 번째만 False, 나머지 True

> keep="last" → 마지막만 False, 나머지 True

> keep=False → 동일 키 전부 True

> **keep은 “True/False가 중복 여부”이지 “삭제할 행”과 1:1로 매칭되는 건 아님** (**`삭제`는 `drop_duplicates()`에서 실행됨)**


### 3-2. `drop_duplicates()` 도 같은 인자
`drop_duplicates(subset=...,keep=...)`는 위 로직을 그대로 "삭제"로 실행합니다.

#### 1) 완전 중복만 제거
```python
people_no_full_dup = people.drop_duplicates() # subset=None, keep="first"
```

#### 2) 키 기준, 최신만 남기고 이전 버전 삭제
실무 최빈 시나리오, `updated_at` 이 가장 최신인 1건만 남기기.
```python
# 1) 최신부터 정렬
people_sorted = people.sort_values("updated_at")  # 오래된→최신, 필요시 ascending=True/False 조정

# 2) 키 기준 마지막만 남기기 → 최신만 유지
people_latest = people_sorted.drop_duplicates(subset=key, keep="last")

# 검증: 키 기준으로 중복 없음
assert people_latest.duplicated(subset=key).sum() == 0
```

#### 3) 모든 중복 행만 따로 보고 싶다면
```python
only_dups = people[ people.duplicated(subset=key, keep=False) ].copy()
```
> 실무 팁

> - "최신만 남기기"는 정렬 기준이 중요합니다. 보통 `updated_at DESC` → keep="first"로도 구현합니다.
  ```python 
  people_latest = (
    people.sort_values("updated_at", ascending=False)
          .drop_duplicates(subset=key, keep="first")
   )
  ```

#### 4) "부분 중복" 병합(통계적 집계)
키는 같지만 수치가 다르면 **비즈니스 룰**로 결합합니다. (예 : 키는 `max`, 몸무게는 `mean`)
```python
agg_rule = {"height": "max", "weight": "mean"}
people_merged = (
    people.groupby(key, as_index=False)
          .agg(agg_rule)
)

# 검증
assert people_merged.duplicated(subset=key).sum() == 0
```
> 주의(SettingWithCopyWarning 회피)

> - "부분 업데이트는 항상 `.loc[조건,"col"] = 값` 형태로 하는게 좋습니다.
> - "중간 필터링 후 수정하려면, `.copy()` 로 명시적 복사본을 만들어 업데이트 하시면 됩니다.

### 마무리
데이터 클리닝의 핵심은 **데이터 타입, 값의 범위, 중복 여부** 를 철저히 점검하고, 상황에 맞는 처리 방식을 선택하는 것입니다.

| 문제 유형  | 주요 원인                  | 처리 방법 예시                                |
| ------ | ---------------------- | --------------------------------------- |
| 타입 오류  | CSV 파싱, 잘못된 입력, 문자열 숫자 | `.astype()`, `.to_datetime()`           |
| 범위 초과  | 입력 실수, 시스템 오류          | 필터링, 하드 리밋, NaN 처리                      |
| 중복 데이터 | 데이터 병합, 입력 중복          | `.drop_duplicates()`, `groupby().agg()` |


<hr>

## 




















































